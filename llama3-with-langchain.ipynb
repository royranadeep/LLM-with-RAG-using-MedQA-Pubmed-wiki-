{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-12T10:05:05.109654Z",
     "iopub.status.busy": "2024-07-12T10:05:05.108810Z",
     "iopub.status.idle": "2024-07-12T10:05:05.530740Z",
     "shell.execute_reply": "2024-07-12T10:05:05.529812Z",
     "shell.execute_reply.started": "2024-07-12T10:05:05.109618Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedQA Dataset parsing for English data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T10:05:11.532755Z",
     "iopub.status.busy": "2024-07-12T10:05:11.532199Z",
     "iopub.status.idle": "2024-07-12T10:05:15.511848Z",
     "shell.execute_reply": "2024-07-12T10:05:15.511048Z",
     "shell.execute_reply.started": "2024-07-12T10:05:11.532724Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def read_question_answer_file(file_path):\n",
    "    \"\"\"Reads a JSONL file with question-answer data and returns a list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))  # Parse each line as JSON\n",
    "    return data\n",
    "\n",
    "# Load your dataset\n",
    "dataset_path = r'C:\\Users\\ranad\\OneDrive - University of Glasgow\\Attachments\\Msc Final Year project\\Data\\MedQA-USMLE-4-options\\phrases_no_exclude_train.jsonl'  # Replace with the path to your JSON file\n",
    "questions_data = read_question_answer_file(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T10:05:17.983043Z",
     "iopub.status.busy": "2024-07-12T10:05:17.982532Z",
     "iopub.status.idle": "2024-07-12T10:05:17.988651Z",
     "shell.execute_reply": "2024-07-12T10:05:17.987532Z",
     "shell.execute_reply.started": "2024-07-12T10:05:17.983007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\n",
      "D\n",
      "{'A': 'Ampicillin', 'B': 'Ceftriaxone', 'C': 'Doxycycline', 'D': 'Nitrofurantoin'}\n"
     ]
    }
   ],
   "source": [
    "print(questions_data[0]['question'])\n",
    "print(questions_data[0]['answer_idx'])\n",
    "print(questions_data[0]['options'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pubmed Data Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install biopython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Fetching the pmids for the query options using entrez</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2210825', '1918224', '3910386', '3073046', '11985490', '27488586', '36208418', '37486410', '3906584', '37474675', '1918222', '1918223', '1918221', '29377708', '22761158', '19496200', '1486190', '15151561', '3433495', '35703558']\n"
     ]
    }
   ],
   "source": [
    "from Bio import Entrez\n",
    "\n",
    "# Replace with your email address\n",
    "Entrez.email = \"2935352R@student.gla.ac.uk\"\n",
    "access_key_id = \"18acd1db794f7de35c1c83811bc106c6a509\"\n",
    "\n",
    "def fetch_pmids(query):\n",
    "  \"\"\"Fetches PMIDs for a given search term.\n",
    "\n",
    "  Args:\n",
    "    search_term: The search term to query Entrez.\n",
    "\n",
    "  Returns:\n",
    "    A list of PMIDs.\n",
    "  \"\"\"\n",
    "  word = query.split()\n",
    "#   if len(word) == 1:\n",
    "#     query = query+\" top 10 records\"\n",
    "  handle = Entrez.esearch(db=\"pubmed\",sort=\"relevance\", term=query, retmax=20,api_key=access_key_id)  # Adjust retmax as needed\n",
    "  record = Entrez.read(handle)\n",
    "  handle.close()\n",
    "\n",
    "  pmids = record[\"IdList\"]\n",
    "  return pmids\n",
    "\n",
    "# Example usage:\n",
    "search_term = \"Ceftriaxone\"\n",
    "pmids = fetch_pmids(search_term)\n",
    "print(pmids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Fetching full text articles for a given PMID</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_bioc_data(pmid):\n",
    "  \"\"\"Fetches BioC data for a list of PMIDs.\n",
    "\n",
    "  Args:\n",
    "    pmids: A list of PMIDs.\n",
    "\n",
    "  Returns:\n",
    "    A list of JSON objects, each representing a BioC document.\n",
    "  \"\"\"\n",
    "\n",
    "  #url = f\"https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pubmed.cgi/BioC_json/{pmid}/unicode\"\n",
    "  url = f\"https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_json/{pmid}/unicode\"\n",
    "  response = requests.get(url)\n",
    "  if response.status_code == 200:\n",
    "     try:\n",
    "      bioc_data = response.json()\n",
    "     except Exception:\n",
    "      #print(f\"Error decoding BioC data for PMID {pmid} (might be non-JSON)\")\n",
    "      bioc_data = None  # Or handle empty data differently\n",
    "  else:\n",
    "     print(f\"Error fetching BioC data for PMID {pmid}: {response.status_code}\")\n",
    "\n",
    "  return bioc_data\n",
    "\n",
    "# # Example usage\n",
    "# pmid = \"35703558\"\n",
    "# bioc_data = fetch_bioc_data(pmid)\n",
    "# print(bioc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data pre-processing and extraction</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "  \"\"\"Removes special characters and extra whitespace from text.\n",
    "\n",
    "  Args:\n",
    "    text: The input text to be cleaned.\n",
    "\n",
    "  Returns:\n",
    "    The cleaned text.\n",
    "  \"\"\"\n",
    "#   special_chars = r\"[\\u03b2\\u00b5\\u03b4\\u03c5\\u03bb\\u0394\\u00f6]\"\n",
    "#   # Remove special characters using regular expression\n",
    "#   text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "# Remove special characters, but keep letters, digits, and single spaces\n",
    "  cleaned_text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "  cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    # Strip leading and trailing spaces (if any)\n",
    "  cleaned_text = cleaned_text.strip()\n",
    "  return cleaned_text\n",
    "\n",
    "#   # Remove extra whitespace\n",
    "#   text = \" \".join(text.split())\n",
    "\n",
    "#   return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_string_to_file(data, filename):\n",
    "  \"\"\"Saves a string to a text file.\n",
    "\n",
    "  Args:\n",
    "    text: The string to be saved.\n",
    "    filename: The name of the file to create.\n",
    "  \"\"\"\n",
    "\n",
    "  with open(filename, \"w\",encoding='utf-8') as f:\n",
    "    json.dump(data, f)\n",
    "\n",
    "# # Example usage:\n",
    "# my_string = \"This is the text I want to save.\"\n",
    "# save_string_to_file(my_string, \"output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename):\n",
    "  \"\"\"Sanitizes a filename by replacing special characters with underscores.\n",
    "\n",
    "  Args:\n",
    "    filename: The original filename.\n",
    "\n",
    "  Returns:\n",
    "    The sanitized filename.\n",
    "  \"\"\"\n",
    "\n",
    "  # Replace non-alphanumeric characters with underscores\n",
    "  filename = re.sub(r'[^\\w]', '_', filename)\n",
    "\n",
    "  # Remove leading and trailing underscores\n",
    "  filename = filename.strip('_')\n",
    "\n",
    "  return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Penicillin plus Ceftriaxone versus Ampicillin plus Ceftriaxone Synergistic Potential against Clinical Enterococcus faecalis Blood Isolates', 'Penicillin plus ceftriaxone is a promising alternative to ampicillin plus ceftriaxone for the treatment of Enterococcus faecalis infective endocarditis Limited data is available supporting the utilization of penicillin plus ceftriaxone A total of 20 E faecalis isolates one wildtype strain JH22 and 19 clinical blood strains were assessed for penicillin plus ceftriaxone and ampicillin plus ceftriaxone synergy using a 24h timekill experiment Susceptibility was determined by broth microdilution Differences in bactericidal bacteriostatic or inactivity as well as synergy between treatments were assessed by chisquare or Fisher exact test All E faecalis isolates were considered susceptible to ampicillin and penicillin Ampicillin plus ceftriaxone versus penicillin plus ceftriaxone similarly demonstrated synergy Bactericidal activity was more commonly observed for ampicillin plus ceftriaxone versus penicillin plus ceftriaxone Among isolates with a penicillin MIC of 4 gmL n 7 synergistic activity for both combinations was less common compared to isolates with a penicillin MIC 2 gmL n 13 Ampicillin plus ceftriaxone and penicillin plus ceftriaxone demonstrate similar synergistic potential against E faecalis clinical blood isolates but strains with higher penicillin and ceftriaxone MICs less frequently demonstrated synergy Further research is warranted to determine the role of the penicillin plus ceftriaxone therapy and the penicillin MIC in clinical practice', 'IMPORTANCE Penicillin plus ceftriaxone demonstrates similar synergistic activity against Enterococcus faecalis to ampicillin plus ceftriaxone Isolates with a penicillin MIC of 4 mgL and a ceftriaxone MIC of 512 or higher lack penicillin plus ceftriaxone synergy despite the penicillin susceptibility MIC breakpoint of 8 mgL', 'Enterococcus faecalis is a leading cause of infective endocarditis IE with a greater than 30 mortality rate Limited treatment options are available due to the intrinsic resistance of E faecalis to the majority of available antibiotics Penicillins eg penicillin ampicillin are one of the few classes of antibiotics active against E faecalis In severe infections such as IE a penicillin alone demonstrates only bacteriostatic activity and leads to an increased rate of treatment failure Therefore combination therapy is essential to achieve synergistic bactericidal activity against E faecalis IE A penicillin plus an aminoglycoside was the first combination utilized and significantly improved mortality Due to rising aminoglycoside resistance and associated nephrotoxicity ampicillin plus ceftriaxone ampicillinceftriaxone is emerging as a preferred alternative therapy', 'Despite E faecalis intrinsic resistance to all cephalosporins including ceftriaxone a synergistic relationship is observed with ampicillin and ceftriaxone via total saturation of penicillinbinding proteins PBP Treatment of E faecalis IE with ampicillinceftriaxone requires a 6week course which presents a challenge when coordinating outpatient parenteral antimicrobial therapy OPAT due to ampicillin stability Historically ampicillin was known to be stable after reconstitution for approximately 8 h at room temperature which led to clinicians prescribing penicillin plus ceftriaxone due to improved penicillin stability 24 h at room temperature despite limited evidence of primary support Recent data demonstrates prolonged stability of ampicillin up to 30 h at room temperature however ampicillin still lacks stability in an elastomeric pump which is an advantageous outpatient intravenous medication delivery system that allows patients the freedom to travel with their medication in their pocket or a pouch without having to have the medication hung superiorly to the infusion pump Penicillin maintains stability in an elastomeric pump thus penicillinceftriaxone is a promising alternative to ampicillinceftriaxone', 'Penicillinceftriaxone requires further evaluation as penicillin is known to have higher MICs compared to ampicillin against E faecalis There are also increasing reports of E faecalis isolates with alterations in essential pbp4 that demonstrate penicillin resistance but are ampicillin susceptible Clinical data is limited in supporting the utilization of penicillinceftriaxone A comparison of penicillinceftriaxone in vitro synergy to ampicillinceftriaxone has only been assessed in checkboard assays which are limited due to the wide variability in result interpretation Synergy assessment via in vitro timekill experiments have not yet been reported We hypothesized that penicillinceftriaxone will have equivalent in vitro synergy to ampicillinceftriaxone against E faecalis blood isolates', 'All isolates were considered susceptible to ampicillin and penicillin Table 1 Wildtype isolate JH22 had an ampicillin MIC of 05 gmL which was lower than previously published MIC of 2 gmL To confirm findings we repeated the assay twice and obtained the same value The penicillin MIC for JH22 was similar to previously published findings', 'Ampicillin Penicillin Ceftriaxone Isolates Broth microdilution Clinical microbiology lab Broth microdilution Clinical microbiology lab Broth microdilution JH22 05 2 256512 e2003 1 2 2 1 128256 e2006 05 2 2 2 16 e2011 1 2 2 2 512 e2012 05 2 2 2 256 e2014 1 2 2 4 512 e2015 1 2 2 1 5121024 e2017 1 2 2 2 5121024 e2020 1 2 2 2 128 e2025 1 2 2 8 128 e2029 05 2 2 4 128 e2031 05 2 1 2 128 e2032 05 2 2 2 256512 e2008 1 2 4 8 2048 e2009 1 2 4 8 2048 e2010 2 2 4 8 512 e2018 1 2 4 8 2048 e2024 1 2 4 4 256 e2027 1 2 4 16 2048 e2028 1 2 4 16 2048', 'All ampicillin MICs were concordant with the clinical microbiology laboratory results Penicillin MICs were within one to two 2fold dilutions of the reported MIC from the clinical microbiology laboratory A total of 13 isolates had a penicillin MIC 2 gmL and seven isolates had a penicillin MIC of 4 gmL All ceftriaxone MICs were elevated as expected due to intrinsic resistance except one isolate that had an MIC of 16 gmL e2006 Most isolates with a penicillin MIC of 4 gmL had a ceftriaxone MIC 2048 gmL n 57 71 compared to most isolates with a penicillin MIC of 2 gmL had a ceftriaxone MIC 512 gmL n 1113 85 Table 1', 'All isolates against ampicillin or penicillin monotherapy at 025xMIC demonstrated inactivity Table 2 Ceftriaxone monotherapy also demonstrated inactivity against all isolates except one isolate which was bacteriostatic e2006 044 054 log10 CFUmL Ampicillin versus penicillin monotherapy at 05xMIC less commonly demonstrated inactivity n 13 65 versus n 19 95 P 004 Ampicillin versus penicillin at 1xMIC similarly did not demonstrate inactivity n 4 20 versus n 2 10 P 066 with majority of isolates demonstrating bactericidal activity against ampicillin n 11 55 and majority of isolates demonstrating bacteriostatic activity against penicillin n 11 55', 'Betalactam monotherapies against E faecalis Timekill assay change in log10 CFUmL from the initial inoculum at 24 h', 'Ceftriaxone alone Ampicillin alone Penicillin alone Isolates 172 mcgmL 025 MIC 05 MIC 1 MIC 025 MIC 05 MIC 1 MIC Isolates with penicillin MIC 2 mcgmL JH22 194 008 205 026 181 018 008 018b 209 006 184 022 126 043b e2003 162 013 206 016 083 037 405 000a 202 008 050 010 384 030a e2006 044 054b 197 003 181 037 331 008a 204 009 198 006 391 011a e2011 252 045 221 007 128 004 414 000a 223 021 208 021 023 022 e2012 108 012 195 006 158 004 016 006 193 006 132 021 323 031a e2014 163 011 175 003 01 018 202 005b 200 010 143 001 190 099b e2015 167 006 157 007 058 051b 412 000a 199 016 148 008 273 012b e2017 172 006 188 008 005 039b 262 054b 120 010 138 015 258 002b e2020 137 012 181 012 123 028 409 000a 205 006 161 002 304 107a e2025 127 007 181 012 090 001 425 005a 165 014 121 002 060 020b e2029 159 001 226 021 177 001 054 004 198 003 060 000 336 076a e2031 198 012 181 011 286 045b 411 000a 224 007 095 034 323 005a e2032 127 007 182 006 150 000 096 001 162 017 044 015 204 107b Isolates with penicillin MIC of 4 mcgmL e2008 211 003 034 005 300 014a 332 005a 103 003 068 013 269 003b ve2009 221 013 106 016 202 009b 408 000a 196 012 135 004 009 014 e2010 205 007 146 005 264 025b 273 008b 172 004 100 010 028 004b e2018 242 002 187 001 146 004 198 009b 222 022 175 006 124 015b e2024 161 004 205 004 123 010 334 002a 178 000 067 081b 383 012a e2027 208 004 158 011 182 001 029 028 181 023 131 003 180 000b e2028 211 012 134 006 020 016b 351 058a 197 000 163 000 127 023b', 'Bactericidal activity was observed defined as 3log10 decrease in CFUmL from initial inoculum', 'Bacteriostatic activity was observed defined as 3log10 decrease in CFUmL from initial inoculum', 'Combination ampicillinceftriaxone versus penicillinceftriaxone similarly demonstrated synergy at 025 05 and 1 MIC n 9 45 versus n 7 35 P 052 n 11 55 versus n 12 60 P 075 and n 5 25 versus n 5 25 P 100 respectively Table 3 Bactericidal activity was more commonly observed for ampicillinceftriaxone versus penicillinceftriaxone at 025 MIC n 9 45 versus n 2 10 P 001 Inactivity was less commonly observed for ampicillinceftriaxone versus penicillinceftriaxone at 05 MIC n 1 5 versus n 7 35 P 004 All other antibacterial activity was similar between the two combinations', 'Betalactam combination therapies against E faecalis Timekill assay change in log10 CFUmL from the initial inoculum at 24 h', 'Ampicillin ceftriaxone Penicillin ceftriaxone Isolate 025 MIC 05 MIC 1 MIC 025 MIC 05 MIC 1 MIC Isolates with penicillin MIC 2 mcgmL JH22 204 012bc 413 000ac 413 000ac 112 008 045 082 301 008a e2003 405 000a 405 000a 405 000a 190 017 37 025ac 405 000a e2006 405 000a 405 000a 405 000a 296 012bc 405 000ac 405 000a e2011 368 009ac 414 000ac 414 000a 160 023 411 005ac 395 010ac e2012 120 032 401 000ac 401 000ac 102 025 394 000ac 392 012a e2014 405 000ac 405 000ac 405 000ac 038 024bc 398 000ac 405 000ac e2015 412 000ac 412 000ac 412 000a 12 012bc 22 029bc 412 000a e2017 231 016bc 399 000ac 399 000a 284 004bc 261 011bc 399 000a e2020 409 000ac 409 000ac 409 000a 016 055b 406 005ac 409 000a e2025 389 000ac 428 000ac 428 000a 201 014bc 406 000ac 428 000ac e2029 173 001 375 045ac 344 089ac 097 068 157 071bc 407 000a e2031 316 044ac 411 000a 411 000a 405 009ac 376 025ac 411 000a e2032 163 012 249 009b 414 000a 182 024 005 019b 414 000ac Isolates with penicillin MIC of 4 mcgmL e2008 040 003b 256 012b 344 004a 100 007 046 010 25 004b e2009 057 025b 261 007b 408 000a 173 009 112 019 215 020bc e2010 019 006 151 006b 335 010a 100 008 070 003 082 000b e2018 146 000 04 002 337 007a 197 023 166 005 149 006b e2024 342 071ac 392 000ac 392 000a 351 002ac 392 000ac 392 000a e2027 118 019 031 013bc 275 044bc 166 002 095 024 212 011b e2028 126 020 121 033b 364 018a 199 003 156 008 127 025b', 'Bactericidal activity was observed defined as 3log10 decrease in CFUmL from initial inoculum', 'Bacteriostatic activity was observed defined as 3log10 decrease in CFUmL from initial inoculum', 'Synergy was detected as indicated by gray shading defined as 2log10 decrease in CFUmL at 24 h from the most active single agent was observed', 'Among isolates with a penicillin MIC of 4 gmL n 7 synergistic activity for ampicillinceftriaxone and penicillinceftriaxone was less common compared to isolates with a penicillin MIC 2 gmL n 13 Table 3 Bactericidal activity was also more common among isolates with a penicillin MIC 2 versus a penicillin MIC of 4 gmL Only one isolate e2024 demonstrated bactericidal activity and synergy at 025 and 05 MIC for both combinations and was the only penicillin MIC of 4 gmL isolate with a lower ceftriaxone MIC of 256 gmL All other isolates with a penicillin MIC of 4 gmL exposed to penicillinceftriaxone at 025 and 05 MIC were inactive and had a higher ceftriaxone MIC range 512 to 2048 gmL', 'Among 20 clinical Enterococcus faecalis blood isolates similar synergistic activity was observed for ampicillinceftriaxone and penicillinceftriaxone combinations More frequent bactericidal activity was demonstrated among isolates treated with ampicillinceftriaxone versus penicillinceftriaxone Penicillinceftriaxone also demonstrated more bactericidal at higher penicillin concentrations Ampicillinceftriaxone and penicillinceftriaxone synergy was less frequently observed among isolates with a higher penicillin MIC of 4 gmL as well as higher ceftriaxone MICs The observed differences in activity may impact clinical outcomes in patients with E faecalis IE especially since lactambased treatment failure has been shown to occur more frequently in patients infected with a penicillinresistant ampicillinsusceptible E faecalis According to CLSI the clinical MIC breakpoint for penicillin and E faecalis is 8 gmL indicating our isolates were still susceptible Interestingly majority of our isolates with a penicillin MIC of 4 gmL had a higher reported MIC by Vitek 2 Table 1 with two isolates having an MIC above the breakpoint ie 16 gmL Our findings of lower MICs by broth microdilution compared to Vitek 2 are similar to a previous report of 49 penicillinresistant ampicillinsusceptible E faecalis isolates which found that 939 of isolate MICs by Vitek 2 were two 2fold dilutions higher than broth microdilution', 'While the prevalence of penicillinresistant ampicillinsusceptible E faecalis remains unknown in the Unites States US lactambased treatments remain the standard of care Ampicillinceftriaxone is the most commonly used firstline combination to treat E faecalis IE due to the improved safety profile compared to aminoglycosidebased treatments as well as rising aminoglycoside resistance up to 60 However due to the challenges of coordinating ampicillin therapy in the outpatient setting patients often receive penicillinceftriaxone Limited clinical data is available supporting the use of penicillinceftriaxone in clinical practice for E faecalis IE and has only been assessed in patients who have already received standard of care treatment The first discussion was a retrospective review in the US that identified five patients who were discharged on penicillinceftriaxone after receiving 3 to 8 days of ampicillinceftriaxone inpatient Two of the five patients were lost to followup and the other three achieved clinical cure with no relapse at 90 days The penicillin MICs were not reported in this study Another case series of four patients in the US who received penicillinceftriaxone after already receiving standard of care ie ampicillinceftriaxone n 3 and penicillin plus gentamicin n 1 treatment for a range of 3 to 32 days reported no recurrence in infection at 6 months The authors reported penicillin MICs for three of the four patients by Etest where two patients had an MIC of 4 gmL and one had an MIC of 2 gmL Etest methodology is found to correlate well to broth microdilution methodology which perhaps suggests that a penicillin MIC of 4 gmL does not always indicate treatment failure as we observed with our isolates However one of the patients received chronic amoxicillin oral suppression and the other patient received penicillingentamicin for 32 days prior to transitioning to penicillinceftriaxone which may falsely make penicillinceftriaxone appear efficacious', 'A larger multicenter case series in New Zealand of 41 patients with enterococcal endocarditis E faecalis n 40 and Enterococcus faecium n 1 compared outpatient treatment with penicillin plus gentamicin n 20 versus penicillinceftriaxone n 23 found no difference in recurrence 11 versus 5 P 059 but a greater incidence of side effects in patients receiving gentamicin therapy 35 versus 0 P 001 Patients received a median of 15 days interquartile range IQR 9185 days of inpatient penicillin amoxicillin amoxicillinclavulanic acid piperacillin or piperacillintazobactam plus a synergy antibiotic ie ceftriaxone or gentamicin prior to discharge The predominant synergy antibiotic received during the first 14 days of treatment was then selected for the patient on discharge Penicillin susceptibility was available for 32 of the isolates with a median MIC of 3 IQR 24 but methodology was not reported Similar efficacy between the two combinations may be due to more patients in the penicillinceftriaxone group receiving chronic amoxicillin oral suppression 5 versus 35 P 002', 'Most recently a singlecenter retrospective cohort study in Australia identified 20 patients with an E faecalis endovascular infection who received penicillinceftriaxone via OPAT from their existing OPAT database Six patients 30 experienced an unplanned readmission one patient 5 had a relapse in bacteremia within 6 months and 1year mortality was 15 All isolates were considered susceptible to penicillin by Vitek 2 but MICs were not reported A random six isolates were selected for testing by broth microdilution and synergy assessment by checkerboard which revealed a median penicillin MIC of 1 gmL IQR 051 gmL and synergy for four isolates against ampicillinceftriaxone and three isolates against penicillinceftriaxone It is important to note however that the interpretation of the fractional inhibitory concentration index FICI was not indicated Overall the clinical data is limited and randomized controlled trials are needed to determine the equivalence of penicillinceftriaxone to ampicillinceftriaxone and the role of the E faecalis penicillin MIC in predicting treatment success', 'Only one other in vitro study has been published to date which also compared checkboard synergy of ampicillinceftriaxone versus penicillinceftriaxone among 28 clinical E faecalis blood isolates from Germany and one wildtype isolate ATCC 29212 The ceftriaxone concentrations utilized included the free plasma trough concentrations for a 2 g IV q12h 4 g IV q24h and 2 g IV q24h regimens which were 4 15 and 1 gL respectively Conversely we utilized a free steadystate plasma concentration of 172 gmL based on clinical pharmacokinetic data for a 2 g IV q12h regimen where the extrapolated trough would be 913 gmL versus the 4 gmL utilized by Thieme et al The difference in concentration is likely due to the variability in patient pharmacokinetics but should be considered when interpreting the in vitro synergy results Additionally the authors utilized three different definitions for the FICI which leads to wide variability in result interpretation When utilizing an FICI 05 to indicate synergy a total of 22 759 and 16 552 isolates were synergistic against ampicillinceftriaxone and penicillinceftriaxone respectively When utilizing the median FICI of 08 as the synergy threshold an additional five isolates n 21 724 demonstrated penicillinceftriaxone synergy and no additional isolates demonstrated ampicillinceftriaxone synergy We observed comparable rates of synergy between the two combinations We also observed minimal synergy for both combinations in isolates with a penicillin MIC of 4 gmL whereas Thieme et al found a strong inverse correlation indicating that the higher the penicillin MIC the lower the FICI rs 061 P 0001 However these results are difficult to interpret as the lower the FICI does not necessarily mean more synergy', 'In addition among our isolates with a penicillin MIC of 4 gmL the ceftriaxone MICs were higher compared to isolates with a penicillin MIC 2 gmL One isolate with a penicillin MIC of 4 gmL e2024 had a ceftriaxone MIC of 256 gmL and was also the only isolate with a penicillin MIC of 4 gmL that demonstrated synergy against both ampicillinceftriaxone and penicillinceftriaxone Similarly Thieme L et als in vitro checkboard study found that isolates with a ceftriaxone MIC 1024 gmL n 4 did not demonstrate synergy for either combination and ceftriaxone concentrations required to reduce the ampicillin or penicillin were frequently unachievable or higher than physiologically achievable ceftriaxone concentrations Therefore we chose to utilize the free plasma steadystate concentrations fCpss of ceftriaxone to improve the clinical applicability of our results similar to previous work Utilization of ceftriaxone at 012 025 and 05 MIC in a timekill assay in previous work with isolate JH22 ceftriaxone MIC 512 gmL also yielded similar synergistic results The relationship of the ceftriaxone MIC along with the penicillin MIC to lactam synergistic potential may be related to changes in essential penicillinbinding protein4 PBP4 Although alterations in pbp4 have only been reported in penicillinresistant ampicillinsusceptible isolates further investigation is warranted to determine if pbp4 mutations are present in isolates with higher penicillin MICs near the breakpoint ie 4 and 8 gmL', 'The strength of our study was the inclusion of clinical blood E faecalis strains and utilization of timekill assays which have a clear synergy definition compared to checkboard methodology While timekill assays are superior to checkboard methodology our results are limited by the static nature of these assays which limits the applicability to determine appropriate dosing that maximizes bactericidal activity To improve applicability to patient care we utilized physiologic concentrations of ceftriaxone but unfortunately the physiologic steadystate plasma concentrations for ampicillin and penicillin were above the MIC and would eradicate the organism without ceftriaxone in combination As a result we utilized subinhibitory concentrations at 025 05 and 1 MIC which led to variability in the concentrations utilized across the isolates Further research is warranted to determine optimal penicillinceftriaxone dosing and the role of the E faecalis penicillin MIC in predicting treatment success', 'Overall ampicillinceftriaxone and penicillinceftriaxone demonstrates similar synergistic potential but ampicillinceftriaxone is more bactericidal Strains with higher penicillin and ceftriaxone MICs less frequently demonstrated synergy with both ampicillinceftriaxone and penicillinceftriaxone Higher penicillin concentrations were warranted to achieve bactericidal activity but further research is warranted to determine the appropriate dose to optimize penicillin exposure', 'A total of 20 E faecalis isolates were included one wildtype strain JH22 and 19 clinical strains from blood Most clinical isolates obtained were ampicillin and penicillin susceptible by the clinical microbiology laboratory Vitek 2 bioMrieux Inc Durham NC There were two isolates that had a penicillin MIC of 16 gmL which is one 2fold dilution above the clinical breakpoint for penicillin is 8 gmL All isolates were stored in CryoCare Stamford TX tryptic soy broth plus glycerol at 80C and were subcultured once on brain heart infusion agar for 1824 h at 35C prior to each experiment', 'Antibiotic powders were purchased from SigmaAldrich Inc St Louis MO ampicillin sodium product number A0166 penicillin G potassium salt product number 46609 and ceftriaxone sodium product number PHR1382 Experiments were performed using cation adjusted calcium 25 gmL magnesium 125 gmL MuellerHinton broth MHB BD Difco Sparks MD All viable cell count samples and subcultures were plated on brain heart infusion agar BHIA BD Difco Sparks MD', 'MICs were performed for ampicillin penicillin and ceftriaxone by broth microdilution to confirm clinical microbiology laboratory results according to CLSI All MICs were performed in duplicate and were repeated to confirm any discordant MIC values between the clinical microbiology laboratory and our laboratory', 'A 24h timekill experiment was utilized to detect synergy for ampicillinceftriaxone versus penicillinceftriaxone against all 20 isolates Experiments were performed in duplicate in a 12well plate with a final volume of 2 mL Assays were repeated to confirm findings if a wide standard deviation in results was observed The starting inoculum was 106 CFUmL and plates were placed in the incubator at 35C at 50 rotations per minute rpm Subinhibitory concentrations 025 and 05 MIC of ampicillin and penicillin were utilized as previously described and 1xMIC was also tested Ceftriaxone was tested at the free plasma steadystate concentration fCpss 172 gmL based on population pharmacokinetic data for a 2 g IV q12h regimen t12 72 h fCmax 289 gmL as subinhibitory concentrations would not be physiologically achievable due to the intrinsic resistance of ceftriaxone to enterococcus Each drug was tested as monotherapy and both ampicillin and penicillin were combined with ceftriaxone Samples were obtained at 0 4 and 24 h and diluted 110 in normal saline to obtain viable cell counts Three 20 mcL samples of each dilution were plated onto BHIA and incubated for 1824 h at 35C where the average of the three samples was taken to obtain a log10 CFUmL viable cell count Samples were directly obtained from the 12well plate if bacterial growth was not visible for a lower limit of detection of 2log10 CFUmL Antimicrobial activity was defined as bacteriostatic or bactericidal which were defined as 3log10 CFUmL or 3log10 CFUmL decrease from initial inoculum at 24 h Inactivity was defined as an increase in log10 CFUmL from initial inoculum at 24 h Combination therapy activity was determined to be synergistic if a 2log10 decrease in CFUmL at 24 h from the most active single agent was observed', 'Differences in bactericidal bacteriostatic or inactivity between ampicillin and penicillin monotherapies and ampicillinceftriaxone and penicillinceftriaxone combinations were assessed by chisquare or Fisher exact test Differences in synergy between ampicillinceftriaxone and penicillinceftriaxone as well as between isolates with a penicillin MIC 2 gmL and isolates with a penicillin MIC of 4 gmL for each combination were assessed by chisquare or Fisher exact test All statistical analyses were performed using R version 412', 'Ampicillin plus ceftriaxone is as effective as ampicillin plus gentamicin for treating enterococcus faecalis infective endocarditis', 'Prospective observational study of the clinical prognoses of patients with bloodstream infections caused by ampicillinsusceptible but penicillinresistant Enterococcus faecalis', 'Contemporary clinical and molecular epidemiology of vancomycinresistant enterococcal bacteremia a prospective multicenter cohort study VENOUS I', 'Infective endocarditis in adults diagnosis antimicrobial therapy and management of complications a scientific statement for healthcare professionals from the American Heart Association', 'A review of combination antimicrobial therapy for Enterococcus faecalis bloodstream infections and infective endocarditis', 'Effect of freezing and microwave thawing on the stability of six antibiotic admixtures in plastic bags', 'Stability of benzylpenicillin potassium and ampicillin in an elastomeric infusion pump', 'Treatment of Enterococcus faecalis infective endocarditis with penicillin G plus ceftriaxone', 'Penicillin as a potential agent for dual betalactam therapy for Enterococcal endocarditis', 'Stability of ampicillin in normal saline and buffered normal saline', 'Ampicillin and ceftriaxone solution stability at different temperatures in outpatient parenteral antimicrobial therapy', 'Spread of an unusual penicillin and imipenemresistant but ampicillinsusceptible phenotype among Enterococcus faecalis clinical isolates', 'Structural and regulatory changes in PBP4 trigger decreased betalactam susceptibility in Enterococcus faecalis', 'Outpatient continuousinfusion benzylpenicillin combined with either gentamicin or ceftriaxone for enterococcal endocarditis', 'A clinical and in vitro assessment of outpatient parenteral benzylpenicillin and ceftriaxone combination therapy for enterococcal endovascular infections', 'Comparison of methods of interpretation of checkerboard synergy testing', 'In Vitro Synergism of Penicillin and Ceftriaxone against Enterococcus faecalis', 'Role of class A penicillinbinding proteins in PBP5mediated betalactam resistance in Enterococcus faecalis', 'Evaluation of Enterococcus faecalis clinical isolates with penicillinresistant ampicillinsusceptible phenotype as reported by Vitek2 Compact system', 'Is oncedaily highdose ceftriaxone plus ampicillin an alternative for Enterococcus faecalis Infective endocarditis in outpatient parenteral antibiotic therapy programs', 'In vitro activity of imipenemrelebactam alone or in combination with amikacin or colistin against Pseudomonas aeruginosa', 'Synergistic effect of amoxicillin and cefotaxime against Enterococcus faecalis', 'Correlation between mutations in liaFSR of Enterococcus faecium and MIC of daptomycin revisiting daptomycin breakpoints', 'Ertapenem pharmacokinetics and impact on intestinal microflora in comparison to those of ceftriaxone after multiple dosing in male and female volunteers']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_text(data):\n",
    "  \"\"\"Extracts text from the given JSON data.\n",
    "\n",
    "  Args:\n",
    "    data: The JSON data.\n",
    "\n",
    "  Returns:\n",
    "    A list of text strings.\n",
    "  \"\"\"\n",
    "\n",
    "  texts = []\n",
    "  for document in data:\n",
    "    for passage in document['documents'][0]['passages']:  # Access the first document's passages\n",
    "        words = passage['text'].split()\n",
    "        if(len(words)> 8):\n",
    "          texts.append(clean_text(passage['text']))\n",
    "  return texts\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have the JSON data stored in a variable named 'json_data'\n",
    "extracted_texts = extract_text(bioc_data)\n",
    "print(extracted_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Saving indivisual options data to json file </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating a for loop to fetch the data.\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "questionLoader = questions_data[327:1000]\n",
    "for question in  tqdm(questionLoader):\n",
    "    options = question['options']\n",
    "    for key, value in options.items():\n",
    "        query = value\n",
    "        bioc_data = []\n",
    "        pmids = fetch_pmids(query)\n",
    "        counter = 0\n",
    "        for pmid in pmids:\n",
    "            data = fetch_bioc_data(pmid)\n",
    "            if data != None:\n",
    "                counter +=1\n",
    "                bioc_data.append(extract_text(data))\n",
    "            time.sleep(0.3)\n",
    "            if counter == 5 : break\n",
    "        save_string_to_file(bioc_data, \"C:/Users/ranad/Documents/Pubmed/\" + sanitize_filename(query) + \".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generate bert embeddings</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ranad\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Initialize the tokenizer and model (bert-base-uncased)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Ensure the model runs on GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model.to(device)\n",
    "\n",
    "\n",
    "# Function to compute embeddings\n",
    "def get_embeddings(texts,tokenizer,model):\n",
    "    \n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    embeddings = last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # Compute the mean of the token embeddings to get a fixed-size representation\n",
    "    embeddings = embeddings.squeeze().cpu().numpy()  # (batch size x hidden size)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dictionary format data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = questions_data[200:327]\n",
    "\n",
    "# Path to the directory containing JSON files\n",
    "pubmed_dir = 'C:/Users/ranad/Documents/Pubmed/'\n",
    "\n",
    "# Dictionary to store the original data (not embeddings) by file key\n",
    "\n",
    "\n",
    "def extract_dataDict(directory):\n",
    "    data_dict = {}\n",
    "    # Iterate through each JSON file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            file_key = sanitize_filename(filename[:-4])  \n",
    "            data_dict[file_key] = data\n",
    "    return data_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = extract_dataDict(pubmed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Osteoarthritis is the most common form of arthritis and a leading cause of disability worldwide largely due to pain the primary symptom of the disease The pain experience in knee osteoarthritis in particular is wellrecognized as typically transitioning from intermittent weightbearing pain to a more persistent chronic pain Methods to validly assess pain in osteoarthritis studies have been developed to address the complex nature of the pain experience The etiology of pain in osteoarthritis is recognized to be multifactorial with both intraarticular and extraarticular risk factors Nonetheless greater insights are needed into pain mechanisms in osteoarthritis to enable rational mechanismbased management of pain Consequences of pain related to osteoarthritis contribute to a substantial socioeconomic burden',\n",
       "  'The hallmark symptom of osteoarthritis OA the most common form of arthritis is pain This is the symptom that drives individuals to seek medical attention and contributes to functional limitations and reduced quality of life Largely because of pain lower extremity OA is wellrecognized as the leading cause of mobility impairment in older adults in the US',\n",
       "  'Approximately 27 million US adults and 85 million UK adults are estimated to have clinical OA defined on the basis of symptoms and physical findings Prevalence of OA increases with age 139 of adults age 25 and older have clinical OA of at least one joint while 336 of adults age 65 and older have OA',\n",
       "  'In large epidemiologic studies OA is often defined on the basis of standard radiographic assessments such as the Kellgren and Lawrence grade Symptomatic OA indicates the presence of both radiographic OA and symptoms ie pain aching stiffness in the same joint attributable to OA as such its prevalence is generally lower than that of radiographic OA ie regardless of symptoms For example the prevalence of radiographic knee OA was 19 and 28 among adults age 45 years in the Framingham study and Johnston County Osteoarthritis Project respectively while the prevalence of symptomatic knee OA was 7 in Framingham and 17 in the Johnston County Osteoarthritis Project The prevalence of symptomatic knee OA in two UK studies ranged from 1119 and estimates of 515 were noted in surveys undertaken in other countries',\n",
       "  'Symptomatic hip OA has been reported to be 9 in the Johnston County Osteoarthritis Project with lower prevalence estimates of 0744 in the UK The prevalence of symptomatic hand OA is higher with the agestandardized prevalence of symptomatic hand OA being 144 and 69 in women and men respectively in younger Framingham cohorts increasing to 262 and 134 respectively among those age 71 in an older Framingham cohort Another study reported an estimate of 8 among adults age 60 and older Incidence of symptomatic hand OA were reported to be 97 for women and 4 for men over a 9year period',\n",
       "  'The lifetime risk of developing symptomatic knee OA is estimated to be 45 40 in men and 47 in women based upon Johnston County Osteoarthritis Project data with risks increasing to 605 among persons who are obese which is approximately double the risk of those who are of normal weight or are underweight With aging of the population and increasing obesity the prevalence of OA is expected to rise Indeed an increase in prevalence of symptomatic knee OA over the past 20 years has been noted in the Framingham cohort rising by 41 and 6 among women and men respectively intriguingly without a concomitant parallel rise in prevalence of radiographic OA Based upon National Health Interview Survey NHIS data the estimated number of US adults with doctordiagnosed arthritis the majority of which is related to OA and likely symptomatic if it has had medical attention is projected to increase to nearly 67 million by 2030',\n",
       "  'Clearly a substantial proportion of adults experience pain related to OA during their lifetime Further individuals with OA in one joint will often have OA in another joints with resulting greater symptomatic burden of the disease',\n",
       "  'The International Association for the Study of Pain defines pain as an unpleasant sensory and emotional experience associated with actual or potential tissue damage or described in terms of such damage It is a complex subjective phenomenon with each individual having a unique perception of it influenced by biological psychological and social factors Under normal circumstances pain is a warning that something is wrong pain from touching a hot stove having injured a joint or chest pain due to ischemia for example In these instances pain plays a protective role signaling to the individual to withdraw from the threat rest to allow tissue healing or seek help etc However once its warning role is over persistence or continued pain ie chronic pain is considered maladaptive',\n",
       "  'Unlike many other pain conditions in which the underlying injury typically heals or resolves OA is a disease that does not resolve Thus OA is typically accompanied by chronic pain Whether and to what degree this ongoing chronic pain i plays an important nociceptive role ii represents maladaptive pain or iii reflects other aspects of the pain experience is not clear',\n",
       "  'The pain experience among persons with OA has been evaluated through a number of qualitative research efforts In the first qualitative study to focus explicitly on pain and related distress as well as changes in pain over time by Hawker et al individuals with hip and knee OA identified two distinct types of OA pain one that was intermittent but generally severe or intense and another that was a persistent background pain or aching Stages of OArelated pain could be discerned with early stages characterized by activityrelated pain becoming more constant over time and punctuated by intermittent intense pain A decrease in participation subsequently occurs in an attempt to avoid triggering such episodes The more intense but less frequent pain that comes and goes ie intermittent particularly when unpredictable had greater impact on quality of life than the background ie constant pain The pain had negative effects on mood participation in social and recreational activities and sleep Similar findings were noted in another study of individuals who had a recent diagnosis of knee OA or were symptomatic but undiagnosed ie prediagnostic knee OA The significance of intermittent knee symptoms was not clear for several years before participants became aware of development of chronic knee symptoms They then altered activities to avoid more symptoms until symptoms affected participation at which time they sought medical care',\n",
       "  'In addition to the concepts of intermittent and constant pain the intensity of daily pain varies widely although the underlying reasons for such variation are not wellunderstood The quality of pain in OA also varies with approximately onethird of individuals with knee OA using descriptors such as burning tingling numbness and pins and needles to characterize their knee symptoms Such descriptors suggest that neuropathic pain may contribute to the OA pain experience although specific nerve lesions have not been identified in OA',\n",
       "  'Given the variation in pain intensity frequency pattern and quality in OA a single simple question about pain is unlikely to adequately capture the full pain experience Some of the variation in reported prevalence of symptomatic OA is related to differences in study design and populations examined but importantly it is also due to the way in which questions about knee pain were formulated Differences in descriptors used to assess pain eg pain vs pain aching or stiffness may elicit different responses Duration over which pain is being assessed eg pain on most days of a month in the past year vs pain on most days of the past month can be prone to recall bias Ideally uniform standardized and valid questionnaires should be used to evaluate pain particularly to enable more precise pain phenotyping and facilitate crossstudy comparisons genetic association studies and drug trial protocol development',\n",
       "  'In OA cohort studies and trials a number of approaches are typically used to assess pain For evaluation of knee OA pain the most common are a visual analog scale VAS or numerical rating scale NRS assessment of pain intensity a single question about presence of pain aching of stiffness in or around the knee over a specified period of time andor the pain subscale of the Western Ontario and McMaster Universities Arthritis Index WOMAC or the Knee injury and Osteoarthritis Outcome Score KOOS The pain subscales of these latter two instruments assess pain experienced with specific activities As a result the pain and function subscale scores are highly correlated Nonetheless these validated instruments are responsive and are used in assessing efficacy of interventions A number of additional validated generic pain instruments are available that are also appropriate for use in OA A metaanalysis concluded that different patientreported outcome measures of pain severity have generally comparable responsiveness to treatment with the singleitem pain assessments with the VAS or NRS resulting in effect estimates comparable to the WOMAC pain subscale although their mean standardized effect sizes were lower To enable meaningful interpretation of response to therapy rather than relying on mean group responses the OMERACTOARSI set of responder criteria were developed and validated for use in clinical trials To be considered a responder at least a minimum threshold of relative and absolute improvement in pain or a lesser degree of absolute and relative improvement in at least 2 out of 3 domains pain function patient global assessment is required Many of these same questions and instruments eg WOMAC can be used for hip OA the Hip disability and Osteoarthritis Outcome Score HOOS is specific for hip OA To assess pain stiffness and physical functioning in hand OA the AustralianCanadian Osteoarthritis Hand Index AUSCAN is commonly used',\n",
       "  'Despite widespread use of these pain assessments the complex pain experience of those living with OA is not adequately captured by existing measures To address this issue a multicenter international OARSIOMERACT initiative led to development of a new measure informed by qualitative research findings that was subsequently validated This new instrument ICOAP Intermittent and Constant OA Pain assesses various facets regarding both intermittent and constant pain for the knee and hip separately including frequency for intermittent pain intensity effects on sleep and quality of life degree of frustration or annoyance and upset or worried feelings associated with the pain as well as whether the intermittent pain occurs without warning or after a trigger The ICOAP has recently been demonstrated to be responsive to change in intervention studies',\n",
       "  'In keeping with the acknowledgement of the multidimensional nature of pain the Initiative on Methods Measurement and Pain Assessment in Clinical Trials IMMPACT has recommended six core domains and associated measures that should be considered when studying any type of chronic pain in clinical trials pain intensity and use of rescue medications physical functioning with a focus on pain interference emotional functioning participant ratings of improvement and satisfaction with treatment symptoms and adverse events and participant disposition Other domains related to pain in OA include fatigue sleep and cognition With the increasing importance of patientreported outcomes the NIHfunded Patient Reported Outcomes Measurement Information System PROMIS provides an opportunity to collect a variety of validated patientreported health outcomes related to physical mental and social wellbeing in addition to pain',\n",
       "  'In view of the complex multidimensional nature of the pain experience in OA it is perhaps not surprising that the underlying etiology of pain is multifactorial most often considered in a biopsychosocial framework Figure 1 A few such risk factors are discussed below',\n",
       "  'The extent to which structural pathology in OA contributes to the pain experience has been controversial A structuresymptom discordance in OA has been widely noted based upon observations of weak correlations between radiographic severity of OA and pain presence or severity although the discordance is less with more severe stages of radiographic disease In a systematic review 1576 of those with knee pain had radiographic OA and 1581 of those with radiographic OA had knee pain The extent of additional xray views obtained the definition of pain symptoms and the nature of the study sample eg age race affected the prevalence of these findings and therefore interpretation of the degree of concordance For example in studies evaluating both the tibiofemoral and patellofemoral joints that also obtained WOMAC pain assessments a more consistent association was noted between pain severity and radiographic OA Supporting such findings a randomized trial demonstrated intraarticular lidocaine to effectively decrease knee pain in comparison with placebo lending further support to the notion that structural pathology within the knee must be contributing to pain',\n",
       "  'Beyond measurement issues there are additional reasons that contribute to an apparent discordance As discussed above pain is a subjective experience influenced by a number of factors including genetic predisposition prior experience expectations about analgesic treatment current mood coping strategies and catastrophizing and sociocultural environment as some examples Without taking into account such factors that can contribute to betweenperson differences assessment of the relation of structure to symptoms will be confounded Unfortunately most such factors that contribute to individual variation in pain cannot be feasibly measured or collected in most studies By adequately controlling for betweenperson differences using a withinperson kneematched approach a strong doseresponse relationship can be demonstrated between radiographic severity and pain presence severity and incidence ie new onset',\n",
       "  'While such studies provide confirmation that structural pathology of OA does indeed contribute to the pain experience radiographs do not provide insight into what particular structural pathologies may contribute to such pain A review elsewhere in this issue examines the structural correlates of pain in greater detailREF In brief based upon MRI studies bone marrow lesions synovitis and effusions appear to have the greatest evidence supporting their relation to pain in OA to date',\n",
       "  'Although such studies have highlighted the importance of structural pathology to pain in OA attempts at structure modification have been largely unsuccessful to date with regards to pain Some recent exceptions include promising pain results from trials evaluating zoledronic acid targeting bone marrow lesions with possible additional bone and cartilage effects and strontium ranelate which may have both bone and cartilage effects',\n",
       "  'Other risk factors for pain in OA may be more amenable to modification Psychological factors are wellrecognized as being correlated with pain in OA and the role of cognitive behavioural therapy is outlined elsewhere in this issue REF Specifically some traits such as catastrophizing coping and selfefficacy may be amenable to intervention While depression anxiety and negative affect among others have been associated with OA pain the causal direction of such relationships is difficult to discern Fluctuation in pain has been linked to fluctuation in psychological factors but whether the pain influences the mood or vice versa is difficult to disentangle Although psychological factors can certainly contribute to a heightened pain experience it is also possible that pain itself can contribute to poor mood Such relationships can only be discerned from longitudinal studies of which there are relatively few to date For example pain from OA contributed to functional limitations and fatigue which in turn contributed to depressed mood and worse pain and function in one study evaluating these complex interrelationships Functional brain imaging studies of OA also demonstrate an important role of affective and motivational aspects of pain that should be addressed to improve effective management of OArelated pain This is particularly important in light of the prevalence and impact of comorbid mood disorders on health outcomes',\n",
       "  'Weight is a potential modifiable factor contributing not only to OA risk but also to pain The effect of obesity on pain may be twofold For the lower extremities the effect of excess weights on symptoms may be due to mechanical loading Increased relative fat mass in obesity may potentially contribute to pain symptoms related to elaboration of adipokines although studies are conflicting in this regard While the mechanism by which obesity contributes to pain may not be clear effects of altering weight on OArelated pain have been studied Observational cohort data was used to demonstrate a lower risk of developing symptomatic knee OA among women who lost 5kg Subsequent randomized trials have noted reductions in pain with 10 weight loss with more substantial effects on pain reduction with greater weight loss Importantly weight gain significantly increases pain highlighting the doseresponse relationship of change in weight with change in pain',\n",
       "  'While not directly modifiable there may be a genetic predisposition to development of chronic pain or experiencing greater pain severity that may provide insight into novel therapeutic targets The availability of large cohort studies with standardized pain and xray data has facilitated genetic association studies to address such hypotheses A functional polymorphism Val158Met in the COMT gene which has been associated with pain sensitivity in other clinical conditions was associated with hip OArelated pain in one cohort study but has not yet been replicated in other cohorts TRPV1 and the PACE4 gene PCSK6 were associated with pain in knee OA in two separate metaanalyses while an association with a SCN9 SNP could not be replicated A missense variant in P2RX7 a target identified through a genomewide screen in mice with assessment of mechanical allodynia has been associated with OArelated pain in one cohort Greater details of genetic determinants of pain can be found elsewhere in this issueREF',\n",
       "  'Another area that may provide potential therapeutic targets is related to risk factors that contribute to the transition from acute to chronic pain in OA which at present is not wellunderstood As noted in the qualitative work described above there is a general progression of symptoms from the early stages of OA with activityrelated eg weightbearing symptoms that appear to be nociceptive in nature to a more persistent constant pain that likely reflects other additional processes such as neurobiological mechanisms Tissue injury andor inflammation as may be seen in OA leads to a decrease in the excitation threshold and an increase in responsiveness to suprathreshold stimuli of peripheral nociceptors ie peripheral sensitization Noxious mechanical stimuli can then evoke exaggerated responses primary hyperalgesia and normally innocuous stimuli such as movement of the joint through its normal range of motion may evoke a pain response allodynia As a result of nociceptor activity after tissue injury or inflammation a number of changes occur in the central nervous system These include changes to dorsal horn transmission neuron receptors leading the transmission neurons to become increasingly responsive to peripheral input central sensitization with reduction in the threshold for mechanically induced pain and an expansion of the receptive field of dorsal horn neurons spatial summation Radiating pain in OA likely reflects this latter phenomenon Once established central sensitization is maintained by lowlevel noxious and even nonnociceptive input from the periphery Such changes in the central nervous system are mainly responsible for the enhanced sensitivity to mechanical stimuli that develops outside the area of the injury secondary hyperalgesia',\n",
       "  'Beyond the clinical observations of hyperalgesia allodynia and radiating pain that suggest a role for sensitization in OArelated pain there are some experimental neurophysiologic findings that also support the presence of sensitization in OA Persons with knee OA experience a greater intensity duration and area of hyperalgesia after intramuscular injection of hypertonic saline compared with controls Lidocaine injected into a painful OA knee resulted in pain reduction in both the injected knee and the untreated contralateral knee supporting central pain modulation in OA Persons with knee OA have higher pain intensities compared with controls to the same level of pressure stimuli as well as lower pressure pain thresholds Other studies have also documented lower pain thresholds in persons with OA compared with controls Temporal summation a progressive increase in discharges of dorsal horn neurons in response to repetitive afferent stimulation thought to reflect central sensitization is increased in persons with painful knee OA compared with agematched healthy controls and the degree of sensitization correlated with pain What pathologies of OA may contribute to peripheral andor central sensitization other risk factors for sensitization and identification of the transition from appropriate nociceptive input to sensitization are important research questions that need to be addressed for improved understanding of pain mechanisms in OA In addition further development and validation of tools to assess sensitization will be necessary to support such research efforts',\n",
       "  'Thus there appears to be substantial opportunities to gain further insights into causes and contributors to pain in OA Such insights in turn will provide opportunities for rational mechanismbased targeting of pain for more efficacious therapeutic management of OA patients',\n",
       "  'Because effective treatment for OA and its related pain is not available to date and the disease can be present for decades the public health impact of OA is substantial on an individual and societal level Figure 1 With the high prevalence of knee OA globally not only is OA a leading cause of disability among older adults in the US but it is among the top 10 causes of disability worldwide In recent estimates of global years lived in disability musculoskeletalrelated conditions ranked second with low back pain neck pain and knee OA being the three most common such conditions and knee OA itself ranked within the top 10 noncommunicable diseases for global disabilityadjusted life years ie years of life lost and years lived with disability',\n",
       "  'Symptoms such as pain stiffness and gelling in OA have clear contributions to functional limitations in OA with welldocumented associations of pain severity with degree of functional limitation While most of the research focus to date has been on the knee or hip symptomatic hand OA has important functional limitations predominantly related to weaker grip strength and activities requiring precise pincer grip or power grip Nonetheless a particular focus on lower extremity OA is warranted given the high prevalence of associated disability In a longitudinal panel survey conducted by the US Census bureau arthritis or rheumatism was the most commonly reported cause of disability and difficulties related to lower extremity functioning or activities were the most commonly reported limitations among all respondents Specifically the most common limitation was in walking 3 city blocks which affected an estimated 225 million US adults and difficulty with climbing stairs affecting an estimated 217 million US adults While not all such individuals have symptomatic knee or hip OA it is likely that OA accounts for a large proportion of these limitations Based upon NHANES III data among persons with OA about 80 have some degree of movement limitation and 25 cannot perform major activities of daily living 11 of adults with knee OA require help with personal care and 14 require help with routine needs Symptomatic knee OA can have less obviously apparent effects on functioning as well For example persons with knee OA have slower walking speeds than those without OA Further those with symptomatic knee OA have a faster decline in gait speed over time than those with either knee OA alone or knee pain alone It is not surprising that knee pain also leads to restrictions in mobility outside of the house impacting upon participation',\n",
       "  'Symptomatic OAs economic impact is also substantial Average direct medical charges related to OA care were estimated to be 2600 per year per individual in 1997 and the total ie direct and indirect annual disease costs were estimated to be 5700 per individual USD FY 2000 Those costs need to be considered in the context of the prevalence of the disease to appreciate the overall societal economic impact OA as a primary diagnosis accounted for 1125 million 223 of all arthritisrelated ambulatory medical care visits in 2006 Further arthritisrelated conditions were the second most common reason for medical visits related to chronic conditions in 2005 second only to hypertension which is asymptomatic In terms of inpatient costs OA was the fifth most expensive condition treated in US hospitals in 2008 with a cost of 40 billion in total national hospital expenditures comprising 35 of the national hospital bill and accounting for 70 of all arthritisrelated inpatient hospitalizations Much of those hospitalizations were related to joint replacement surgery Pain is clearly among the main reasons for individuals seeking joint replacement Knee replacement surgeries are one of the most commonly performed orthopedic procedures in the US with 50 of all joint arthoplasties performed on the knee and 97 of those are performed for knee OA In 2004 478000 knee replacement surgeries were performed representing a 3fold increase since 1991 with total hospitalization charges of 1426 billion in 2004 This increase exceeds expectations based upon overall population growth and increase in the proportion of the population that is elderly andor obese The demand for primary total knee replacement is expected to grow by 673 to 348 million procedures by 2030 Adding to these costs is the increase in health care utilization in the two years preceding the surgery',\n",
       "  'To appreciate the total economic burden of OA on society indirect or productivity costs must also be examined Productivity costs typically reflect costs due to lost productivity while being present at work costs due to absence from work and costs for compensation of household work by others Unfortunately there are significant variations among indirect cost studies in OA regarding methodology cost estimation and cost presentation limiting ones ability to determine the magnitude of OAs economic impact For example in one review indirect costs of OA per patient per year varied from 831 in Hong Kong to 12789 in Canada costs in 2006 USD Considering the prevalence of OA workrelated OA costs have been estimated to range from 34 to 132 billion per year Estimates from 1999 indicate that adults with knee OA reported more than 13 days of lost work due to health issues Using a more recent large US employer benefits database those with OA had an average of 63 days of absenteeism compared with 37 days among a matched comparator group with mean total direct and indirect costs being 2 to 3fold higher Similar findings were noted in a Swedish populationbased cohort in which those with physiciandiagnosed knee OA had a 2fold increased risk of sick leave and 4050 increased risk of disability pension compared with the general population Further 2 of all sick days in the population were attributable to knee OA In a systematic literature review regarding work participation occupational limitations and reduced work capacity or job effectiveness were reported more frequently in those with OA than by controls Aggregate annual absenteeism costs of OA were estimated to be 10 billion from the US Medical Expenditure Panel Survey higher than many other major chronic diseases Taking into account both productivity costs and medical costs among adults with paid employment in a study from the Netherlands the total economic burden of knee OA was estimated to be 871 per person per month with the majority of the costs being related to productivity Regardless of the methodologic differences issues with cost estimation and difficulties in comparing costs across studies it is clear that OA has a tremendous economic impact that will only continue to grow with its rising prevalence',\n",
       "  'OA is highly prevalent worldwide with a tremendous symptomatic and economic global burden Although a number of risk factors have been identified for pain in OA the research focus to date has primarily been on structural targets Pharmacologic treatment options remain limited and nonpharmacologic options are underutilized An expansion of the research agenda to more fully explore pain mechanisms operational in OA is urgently needed to enable comprehensive mechanismbased pain management strategies in this prevalent disabling and costly disease',\n",
       "  'This is a PDF file of an unedited manuscript that has been accepted for publication As a service to our customers we are providing this early version of the manuscript The manuscript will undergo copyediting typesetting and review of the resulting proof before it is published in its final citable form Please note that during the production process errors may be discovered which could affect the content and all legal disclaimers that apply to the journal pertain',\n",
       "  'The natural history of disability and its determinants in adults with lower limb musculoskeletal pain',\n",
       "  'Healthrelated quality of life and health service use among older adults with osteoarthritis',\n",
       "  'Prevalence of disabilities and associated health conditions among adults United States 1999',\n",
       "  'The effects of specific medical conditions on the functional limitations of elders in the Framingham Study',\n",
       "  'Estimates of the prevalence of arthritis and other rheumatic conditions in the United States Part II',\n",
       "  'The prevalence of knee osteoarthritis in the elderly The Framingham Osteoarthritis Study',\n",
       "  'Prevalence of knee symptoms and radiographic and symptomatic knee osteoarthritis in African Americans and Caucasians the Johnston County Osteoarthritis Project',\n",
       "  'Knee pain and osteoarthritis in older adults a review of community burden and current use of primary health care',\n",
       "  'Prevalence of hip symptoms and radiographic and symptomatic hip osteoarthritis in African Americans and Caucasians the Johnston County Osteoarthritis Project',\n",
       "  'Prevalence incidence and progression of hand osteoarthritis in the general population the Framingham Osteoarthritis Study',\n",
       "  'Prevalence of symptomatic hand osteoarthritis and its impact on functional status among the elderly The Framingham Study',\n",
       "  'Symptomatic hand osteoarthritis in the United States prevalence and functional impairment estimates from the third US National Health and Nutrition Examination Survey 19911994',\n",
       "  'Increasing prevalence of knee pain and symptomatic knee osteoarthritis survey and cohort data',\n",
       "  'Projections of US prevalence of arthritis and associated activity limitations',\n",
       "  'Understanding the pain experience in hip and knee osteoarthritisan OARSIOMERACT initiative',\n",
       "  'Being careful a grounded theory of emergent chronic knee problems',\n",
       "  'Daily pain variations among patients with hand hip and knee osteoarthritis',\n",
       "  'Validation study of WOMAC a health status instrument for measuring clinically important patient relevant outcomes to antirheumatic drug therapy in patients with osteoarthritis of the hip or knee',\n",
       "  'The Knee injury and Osteoarthritis Outcome Score KOOS from joint injury to osteoarthritis',\n",
       "  'Measures of adult pain Visual Analog Scale for Pain VAS Pain Numeric Rating Scale for Pain NRS Pain McGill Pain Questionnaire MPQ ShortForm McGill Pain Questionnaire SFMPQ Chronic Pain Grade Scale CPGS Short Form36 Bodily Pain Scale SF36 BPS and Measure of Intermittent and Constant Osteoarthritis Pain ICOAP',\n",
       "  'Outcome measures in placebocontrolled trials of osteoarthritis responsiveness to treatment effects in the REPORT database',\n",
       "  'OMERACTOARSI initiative Osteoarthritis Research Society International set of responder criteria for osteoarthritis clinical trials revisited',\n",
       "  'Hip disability and osteoarthritis outcome score An extension of the Western Ontario and McMaster Universities Osteoarthritis Index',\n",
       "  'Clinimetric properties of the AUSCAN Osteoarthritis Hand Index an evaluation of reliability validity and responsiveness',\n",
       "  'Development and preliminary psychometric testing of a new OA pain measurean OARSIOMERACT initiative',\n",
       "  'Responsiveness of the OARSIOMERACT osteoarthritis pain and function measures',\n",
       "  'Core outcome measures for chronic pain clinical trials IMMPACT recommendations',\n",
       "  'Core outcome domains for chronic pain clinical trials IMMPACT recommendations',\n",
       "  'Relationship between symptoms and structural change in osteoarthritis what are the important targets for osteoarthritis therapy',\n",
       "  'Correlates of knee pain among US adults with and without radiographic knee osteoarthritis',\n",
       "  'Analysis of the discordance between radiographic changes and knee pain in osteoarthritis of the knee',\n",
       "  'Epidemiologic associations of pain in osteoarthritis of the knee data from the National Health and Nutrition Examination Survey and the National Health and Nutrition ExaminationI Epidemiologic Followup Survey',\n",
       "  'Determinants of pain severity in knee osteoarthritis effect of demographic and psychosocial variables using 3 pain measures',\n",
       "  'Association of radiographic features of osteoarthritis of the knee with knee pain data from the Baltimore Longitudinal Study of Aging',\n",
       "  'The discordance between clinical and radiographic knee osteoarthritis a systematic search and summary of the literature',\n",
       "  'Symptoms and radiographic osteoarthritis not as discordant as they are made out to be',\n",
       "  'Associations between pain function and radiographic features in osteoarthritis of the knee',\n",
       "  'Pain mechanisms in osteoarthritis of the knee effect of intraarticular anesthetic',\n",
       "  'Regional mu opioid receptor regulation of sensory and affective dimensions of pain',\n",
       "  'The genetic mediation of individual differences in sensitivity to pain and its inhibition',\n",
       "  'A comparison of placebo effects in clinical analgesic trials versus studies of placebo analgesia',\n",
       "  'Expectations and anxiety as mediators of placebo effects in pain',\n",
       "  'Effects of odors on pain perception deciphering the roles of emotion and attention',\n",
       "  'Social environment moderates the association between catastrophizing and pain among persons with a spinal cord injury',\n",
       "  'Is emotional disturbance a precipitator or a consequence of chronic pain',\n",
       "  'Relationship between social desirability and selfreport in chronic pain patients',\n",
       "  'Association between radiographic features of knee osteoarthritis and pain results from two cohort studies',\n",
       "  'Radiographic osteoarthritis severity is associated with an increased risk of developing knee pain Findings from the Osteoarthritis Initiative',\n",
       "  'Do knee abnormalities visualised on MRI explain knee pain in knee osteoarthritis A systematic review',\n",
       "  'Zoledronic acid reduces knee pain and bone marrow lesions over 1 year a randomised controlled trial',\n",
       "  'Efficacy and safety of strontium ranelate in the treatment of knee osteoarthritis results of a doubleblind randomised placebocontrolled trial',\n",
       "  'Pain coping skills training and lifestyle behavioral weight management in patients with knee osteoarthritis a randomized controlled study',\n",
       "  'Pain coping skills training for patients with elevated pain catastrophizing who are scheduled for knee arthroplasty a quasiexperimental study',\n",
       "  'Racial differences in osteoarthritis pain and function potential explanatory factors',\n",
       "  'Negative affect pain and disability in osteoarthritis patients the mediating role of muscle weakness',\n",
       "  'A longitudinal study to explain the paindepression link in older adults with osteoarthritis',\n",
       "  'Psychophysical and functional imaging evidence supporting the presence of central sensitization in a cohort of osteoarthritis patients',\n",
       "  'Arthritic pain is processed in brain areas concerned with emotions and fear',\n",
       "  'Metabolic factors in osteoarthritis obese people do not walk on their hands',\n",
       "  'Weight loss reduces the risk for symptomatic knee osteoarthritis in women The Framingham Study',\n",
       "  'Weight loss as treatment for knee osteoarthritis symptoms in obese patients 1year results from a randomised controlled trial',\n",
       "  'Exercise and dietary weight loss in overweight and obese older adults with knee osteoarthritis the Arthritis Diet and Activity Promotion Trial',\n",
       "  'The Intenstive Diet and Exercise for Arthritis Trial 18month clinical outcomes',\n",
       "  'Benefits of massive weight loss on symptoms systemic inflammation and cartilage turnover in obese patients with knee osteoarthritis',\n",
       "  'Body weight changes and corresponding changes in pain and function in persons with symptomatic knee osteoarthritis A cohort study',\n",
       "  'A functional polymorphism in the catecholOmethyltransferase gene is associated with osteoarthritisrelated pain',\n",
       "  'The Ile585Val TRPV1 variant is involved in risk of painful knee osteoarthritis',\n",
       "  'A role for PACE4 in osteoarthritis pain evidence from human genetic association and null mutant phenotype',\n",
       "  'Role of the Nav1 7 R1150W amino acid change in susceptibility to symptomatic knee osteoarthritis and multiple regional pain',\n",
       "  'Genetically determined P2X7 receptor pore formation regulates variability in chronic pain sensitivity',\n",
       "  'Signaling pathways in sensitization toward a nociceptor cell biology',\n",
       "  'Evidence for a central component of postinjury pain hypersensitivity',\n",
       "  'Different patterns of hyperalgesia induced by experimental inflammation in human skin',\n",
       "  'Osteoarthritis and its association with muscle hyperalgesia an experimental controlled study',\n",
       "  'Lessons from fibromyalgia abnormal pain sensitivity in knee osteoarthritis',\n",
       "  'Impact of nervous system hyperalgesia on pain disability and quality of life in patients with knee osteoarthritis a controlled analysis',\n",
       "  'The reliability and validity of pain threshold measurements in osteoarthritis of the knee',\n",
       "  'Quantitative sensory testing in painful osteoarthritis a systematic review and metaanalysis',\n",
       "  'Years lived with disability YLDs for 1160 sequelae of 289 diseases and injuries 19902010 a systematic analysis for the Global Burden of Disease Study 2010',\n",
       "  'Disabilityadjusted life years DALYs for 291 diseases and injuries in 21 regions 19902010 a systematic analysis for the Global Burden of Disease Study 2010',\n",
       "  'Predicting the course of functional limitation among older adults with knee pain do local signs symptoms and radiographs add anything to general indicators',\n",
       "  'Factors associated with functional impairment in symptomatic knee osteoarthritis',\n",
       "  'Prevalence of symptomatic hand osteoarthritis and its impact on functional status among the elderly The Framingham Study',\n",
       "  'Prevalence and Most Common Causes of Disability Among Adults United States 2005',\n",
       "  'A study of the gait characteristics of patients with chronic osteoarthritis of the knee',\n",
       "  'Is symptomatic knee osteoarthritis a risk factor for a fast decline in gait speed Results from the Osteoarthritis Initiative',\n",
       "  'Factors associated with restricted mobility outside the home in communitydwelling adults ages fifty years and older with knee pain an example of use of the International Classification of Functioning to investigate participation restriction',\n",
       "  'The economic burden associated with osteoarthritis rheumatoid arthritis and hypertension a comparative study',\n",
       "  'The National Hospital Bill The Most Expensive Conditions by Payer 2008',\n",
       "  'Prevalence of primary and revision total hip and knee arthroplasty in the United States from 1990 through 2002',\n",
       "  'Projections of primary and revision hip and knee arthroplasty in the United States from 2005 to 2030',\n",
       "  'Patterns of pharmacotherapy and health care utilization and costs prior to total hip or total knee replacement in patients with osteoarthritis',\n",
       "  'Productivity costs and medical costs among working patients with knee osteoarthritis',\n",
       "  'The need for standardization a literature review of indirect costs of rheumatoid arthritis and osteoarthritis',\n",
       "  'Direct and indirect economic costs among privatesector employees with osteoarthritis',\n",
       "  'Risk of sick leave and disability pension in workingage women and men with knee osteoarthritis',\n",
       "  'The effect of osteoarthritis of the hip or knee on work participation',\n",
       "  'Osteoarthritis and absenteeism costs evidence from US National Survey Data',\n",
       "  'Schematic illustrating the multifactorial nature of pain in OA with complex interrelationships between various risk factors and the potential wideranging effects of OA pain']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['Arthralgias']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Document text arrangement to search using FIASS Index.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 503/503 [00:05<00:00, 90.23it/s]\n"
     ]
    }
   ],
   "source": [
    "medQA_Filtered_data = []\n",
    "\n",
    "for key, data in tqdm(data_dict.items()):\n",
    "        for texts in data:\n",
    "            for text in texts:\n",
    "                if len(text.split())>10:\n",
    "                    medQA_Filtered_data.append(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129477"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(medQA_Filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/503 [00:00<?, ?it/s]Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "  0%|▏                                                                             | 1/503 [02:34<21:36:22, 154.95s/it]Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "  0%|▎                                                                             | 2/503 [07:43<34:06:16, 245.06s/it]Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n",
      "  0%|▎                                                                             | 2/503 [13:12<55:09:42, 396.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m                 embeddings\u001b[38;5;241m.\u001b[39mappend(embedding)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings_dict,embeddings\n\u001b[1;32m---> 19\u001b[0m file_dict_embeddings,fiass_embeddings \u001b[38;5;241m=\u001b[39m embeddings_format(data_dict)\n",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m, in \u001b[0;36membeddings_format\u001b[1;34m(data_dict)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m array \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m     12\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m array:\n\u001b[1;32m---> 13\u001b[0m                 embedding \u001b[38;5;241m=\u001b[39m get_embeddings(text,bert_tokenizer, bert_model)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#                 embeddings_dict[key].append(embedding)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m                 embeddings\u001b[38;5;241m.\u001b[39mappend(embedding)\n",
      "Cell \u001b[1;32mIn[13], line 21\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(text, tokenizer, model)\u001b[0m\n\u001b[0;32m     18\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No need to compute gradients\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     23\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Compute the mean of the token embeddings to get a fixed-size representation\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1142\u001b[0m     embedding_output,\n\u001b[0;32m   1143\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1144\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1145\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1146\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1147\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1148\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1149\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1150\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1151\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1152\u001b[0m )\n\u001b[0;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    685\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m         output_attentions,\n\u001b[0;32m    692\u001b[0m     )\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    695\u001b[0m         hidden_states,\n\u001b[0;32m    696\u001b[0m         attention_mask,\n\u001b[0;32m    697\u001b[0m         layer_head_mask,\n\u001b[0;32m    698\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    699\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    700\u001b[0m         past_key_value,\n\u001b[0;32m    701\u001b[0m         output_attentions,\n\u001b[0;32m    702\u001b[0m     )\n\u001b[0;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:626\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    623\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    624\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 626\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[0;32m    628\u001b[0m )\n\u001b[0;32m    629\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:239\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:638\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 638\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m    639\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:538\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 538\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate through the data dictionary and generate embeddings\n",
    "# Now, convert the original text data in `data_dict` to embeddings\n",
    "# Dictionary to store embeddings by file key\n",
    "from tqdm import tqdm\n",
    "\n",
    "def embeddings_format(data_dict):\n",
    "    embeddings_dict = {}\n",
    "    embeddings = []\n",
    "    for key, data in tqdm(data_dict.items()):\n",
    "        embeddings_dict[key] = []\n",
    "        for array in data:\n",
    "            for text in array:\n",
    "                embedding = get_embeddings(text,bert_tokenizer, bert_model)\n",
    "#                 embeddings_dict[key].append(embedding)\n",
    "                embeddings.append(embedding)\n",
    "    return embeddings_dict,embeddings\n",
    "                \n",
    "\n",
    "file_dict_embeddings,fiass_embeddings = embeddings_format(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'194_200_json': []}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_dict_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIASS Installation and Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu \n",
    "#pip install faiss-gpu  # For GPU support, if you have a CUDA-capable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Get the dimensionality of the embeddings\n",
    "embedding_dim = embeddings.shape[1]\n",
    "\n",
    "# Create a FAISS index\n",
    "# IndexFlatL2 is a simple, exact nearest neighbor search index with L2 distance\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "# Check the number of vectors in the index\n",
    "print(f\"Number of vectors in the index: {index.ntotal}\")\n",
    "\n",
    "# Save the index to a file for later use\n",
    "faiss.write_index(index, 'FAISS\\QA200_327_index.index')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FAISS index\n",
    "faiss_index = faiss.read_index('FAISS\\QA200_327_index.index')\n",
    "\n",
    "# Function to search the FAISS index\n",
    "def search_faiss_index(query, index, tokenizer, model, top_k=5):\n",
    "    # Compute the query embedding\n",
    "    query_embedding = get_embeddings(query, tokenizer, model)\n",
    "\n",
    "    # Reshape the query embedding to match FAISS expected input format\n",
    "    query_embedding = np.expand_dims(query_embedding, axis=0).astype('float32')\n",
    "\n",
    "    # Search the FAISS index\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    return distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = \"Sample query text\"\n",
    "top_k = 5\n",
    "\n",
    "distances, indices = search_faiss_index(query, index, bert_tokenizer, bert_model, top_k=top_k)\n",
    "\n",
    "# Print results\n",
    "print(f\"Top-{top_k} results:\")\n",
    "for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "    print(f\"Result {i + 1}:\")\n",
    "    print(f\"Index: {idx}, Distance: {distance}\")\n",
    "    # If you have the original text stored, you can retrieve it like this:\n",
    "    # print(f\"Text: {embeddings_dict['some_key'][idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T10:05:21.030809Z",
     "iopub.status.busy": "2024-07-12T10:05:21.029959Z",
     "iopub.status.idle": "2024-07-12T10:07:13.865645Z",
     "shell.execute_reply": "2024-07-12T10:07:13.864647Z",
     "shell.execute_reply.started": "2024-07-12T10:05:21.030777Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.2.12-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (3.8.5)\n",
      "Collecting langchain-core<0.3.0,>=0.2.27 (from langchain)\n",
      "  Downloading langchain_core-0.2.29-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.98-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (2.5.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.27->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<0.3.0,>=0.2.27->langchain)\n",
      "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (4.9.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.6-cp311-none-win_amd64.whl.metadata (51 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.14.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain) (2.1)\n",
      "Downloading langchain-0.2.12-py3-none-any.whl (990 kB)\n",
      "   ---------------------------------------- 0.0/990.6 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 524.3/990.6 kB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 990.6/990.6 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.2.29-py3-none-any.whl (383 kB)\n",
      "Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.98-py3-none-any.whl (140 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading orjson-3.10.6-cp311-none-win_amd64.whl (136 kB)\n",
      "Downloading packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Installing collected packages: packaging, orjson, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "Successfully installed jsonpatch-1.33 langchain-0.2.12 langchain-core-0.2.29 langchain-text-splitters-0.2.2 langsmith-0.1.98 orjson-3.10.6 packaging-24.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "python-lsp-black 1.2.1 requires black>=22.3.0, but you have black 0.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ranad\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: boto3 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from transformers) (1.26.76)\n",
      "Requirement already satisfied: requests in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: regex in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Collecting sentencepiece (from transformers)\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.76 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from boto3->transformers) (1.29.76)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: six in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from tqdm->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from botocore<1.30.0,>=1.29.76->boto3->transformers) (2.8.2)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 524.3/991.5 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 991.5/991.5 kB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from accelerate) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from accelerate) (2.2.0)\n",
      "Collecting huggingface-hub>=0.21.0 (from accelerate)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.3.1 (from accelerate)\n",
      "  Downloading safetensors-0.4.4-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.9.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.21.0->accelerate)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "Downloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "Downloading safetensors-0.4.4-cp311-none-win_amd64.whl (285 kB)\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, accelerate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed accelerate-0.33.0 fsspec-2024.6.1 huggingface-hub-0.24.5 safetensors-0.4.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2023.4.0 requires fsspec==2023.4.0, but you have fsspec 2024.6.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.43.3-py3-none-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from bitsandbytes) (2.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from bitsandbytes) (1.25.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from jinja2->torch->bitsandbytes) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.43.3-py3-none-win_amd64.whl (136.5 MB)\n",
      "   ---------------------------------------- 0.0/136.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/136.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/136.5 MB 2.6 MB/s eta 0:00:52\n",
      "   ---------------------------------------- 1.3/136.5 MB 2.2 MB/s eta 0:01:01\n",
      "    --------------------------------------- 2.1/136.5 MB 2.6 MB/s eta 0:00:52\n",
      "    --------------------------------------- 2.6/136.5 MB 2.8 MB/s eta 0:00:48\n",
      "    --------------------------------------- 2.9/136.5 MB 2.3 MB/s eta 0:00:58\n",
      "    --------------------------------------- 3.4/136.5 MB 2.3 MB/s eta 0:00:59\n",
      "   - -------------------------------------- 3.9/136.5 MB 2.3 MB/s eta 0:00:57\n",
      "   - -------------------------------------- 4.2/136.5 MB 2.4 MB/s eta 0:00:56\n",
      "   - -------------------------------------- 4.5/136.5 MB 2.3 MB/s eta 0:00:59\n",
      "   - -------------------------------------- 5.0/136.5 MB 2.2 MB/s eta 0:01:01\n",
      "   - -------------------------------------- 5.2/136.5 MB 2.1 MB/s eta 0:01:03\n",
      "   - -------------------------------------- 6.0/136.5 MB 2.2 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 6.3/136.5 MB 2.2 MB/s eta 0:01:01\n",
      "   -- ------------------------------------- 7.1/136.5 MB 2.2 MB/s eta 0:00:58\n",
      "   -- ------------------------------------- 7.9/136.5 MB 2.3 MB/s eta 0:00:56\n",
      "   -- ------------------------------------- 8.4/136.5 MB 2.3 MB/s eta 0:00:56\n",
      "   -- ------------------------------------- 8.7/136.5 MB 2.3 MB/s eta 0:00:56\n",
      "   -- ------------------------------------- 9.2/136.5 MB 2.3 MB/s eta 0:00:56\n",
      "   -- ------------------------------------- 9.4/136.5 MB 2.3 MB/s eta 0:00:57\n",
      "   -- ------------------------------------- 9.7/136.5 MB 2.2 MB/s eta 0:00:58\n",
      "   -- ------------------------------------- 10.2/136.5 MB 2.2 MB/s eta 0:00:58\n",
      "   --- ------------------------------------ 10.7/136.5 MB 2.2 MB/s eta 0:00:57\n",
      "   --- ------------------------------------ 11.3/136.5 MB 2.3 MB/s eta 0:00:56\n",
      "   --- ------------------------------------ 11.5/136.5 MB 2.2 MB/s eta 0:00:57\n",
      "   --- ------------------------------------ 12.1/136.5 MB 2.2 MB/s eta 0:00:57\n",
      "   --- ------------------------------------ 13.1/136.5 MB 2.3 MB/s eta 0:00:54\n",
      "   --- ------------------------------------ 13.4/136.5 MB 2.3 MB/s eta 0:00:55\n",
      "   --- ------------------------------------ 13.6/136.5 MB 2.2 MB/s eta 0:00:55\n",
      "   ---- ----------------------------------- 14.4/136.5 MB 2.3 MB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 15.2/136.5 MB 2.3 MB/s eta 0:00:53\n",
      "   ---- ----------------------------------- 16.0/136.5 MB 2.4 MB/s eta 0:00:51\n",
      "   ---- ----------------------------------- 16.5/136.5 MB 2.4 MB/s eta 0:00:50\n",
      "   ---- ----------------------------------- 16.8/136.5 MB 2.4 MB/s eta 0:00:51\n",
      "   ----- ---------------------------------- 17.3/136.5 MB 2.4 MB/s eta 0:00:51\n",
      "   ----- ---------------------------------- 17.8/136.5 MB 2.3 MB/s eta 0:00:51\n",
      "   ----- ---------------------------------- 18.6/136.5 MB 2.4 MB/s eta 0:00:50\n",
      "   ----- ---------------------------------- 19.1/136.5 MB 2.4 MB/s eta 0:00:49\n",
      "   ----- ---------------------------------- 19.9/136.5 MB 2.4 MB/s eta 0:00:48\n",
      "   ------ --------------------------------- 20.7/136.5 MB 2.5 MB/s eta 0:00:47\n",
      "   ------ --------------------------------- 21.8/136.5 MB 2.5 MB/s eta 0:00:46\n",
      "   ------ --------------------------------- 22.0/136.5 MB 2.5 MB/s eta 0:00:46\n",
      "   ------ --------------------------------- 22.5/136.5 MB 2.5 MB/s eta 0:00:46\n",
      "   ------ --------------------------------- 23.3/136.5 MB 2.5 MB/s eta 0:00:45\n",
      "   ------- -------------------------------- 24.1/136.5 MB 2.6 MB/s eta 0:00:44\n",
      "   ------- -------------------------------- 25.2/136.5 MB 2.6 MB/s eta 0:00:43\n",
      "   ------- -------------------------------- 25.4/136.5 MB 2.6 MB/s eta 0:00:44\n",
      "   ------- -------------------------------- 26.2/136.5 MB 2.6 MB/s eta 0:00:43\n",
      "   ------- -------------------------------- 27.0/136.5 MB 2.6 MB/s eta 0:00:42\n",
      "   -------- ------------------------------- 27.5/136.5 MB 2.6 MB/s eta 0:00:42\n",
      "   -------- ------------------------------- 28.0/136.5 MB 2.6 MB/s eta 0:00:42\n",
      "   -------- ------------------------------- 29.1/136.5 MB 2.7 MB/s eta 0:00:41\n",
      "   -------- ------------------------------- 29.9/136.5 MB 2.7 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 30.4/136.5 MB 2.7 MB/s eta 0:00:40\n",
      "   --------- ------------------------------ 30.9/136.5 MB 2.7 MB/s eta 0:00:40\n",
      "   --------- ------------------------------ 31.5/136.5 MB 2.7 MB/s eta 0:00:40\n",
      "   --------- ------------------------------ 32.0/136.5 MB 2.7 MB/s eta 0:00:40\n",
      "   --------- ------------------------------ 32.8/136.5 MB 2.7 MB/s eta 0:00:39\n",
      "   --------- ------------------------------ 33.3/136.5 MB 2.7 MB/s eta 0:00:39\n",
      "   --------- ------------------------------ 33.8/136.5 MB 2.7 MB/s eta 0:00:39\n",
      "   ---------- ----------------------------- 34.6/136.5 MB 2.7 MB/s eta 0:00:38\n",
      "   ---------- ----------------------------- 34.9/136.5 MB 2.7 MB/s eta 0:00:38\n",
      "   ---------- ----------------------------- 35.1/136.5 MB 2.6 MB/s eta 0:00:39\n",
      "   ---------- ----------------------------- 35.9/136.5 MB 2.7 MB/s eta 0:00:38\n",
      "   ---------- ----------------------------- 36.4/136.5 MB 2.7 MB/s eta 0:00:38\n",
      "   ---------- ----------------------------- 36.7/136.5 MB 2.7 MB/s eta 0:00:38\n",
      "   ---------- ----------------------------- 37.2/136.5 MB 2.6 MB/s eta 0:00:38\n",
      "   ----------- ---------------------------- 38.3/136.5 MB 2.7 MB/s eta 0:00:37\n",
      "   ----------- ---------------------------- 38.5/136.5 MB 2.7 MB/s eta 0:00:37\n",
      "   ----------- ---------------------------- 38.8/136.5 MB 2.6 MB/s eta 0:00:37\n",
      "   ----------- ---------------------------- 39.3/136.5 MB 2.6 MB/s eta 0:00:37\n",
      "   ----------- ---------------------------- 39.8/136.5 MB 2.6 MB/s eta 0:00:37\n",
      "   ----------- ---------------------------- 40.6/136.5 MB 2.6 MB/s eta 0:00:37\n",
      "   ------------ --------------------------- 41.4/136.5 MB 2.7 MB/s eta 0:00:36\n",
      "   ------------ --------------------------- 41.9/136.5 MB 2.6 MB/s eta 0:00:36\n",
      "   ------------ --------------------------- 42.7/136.5 MB 2.7 MB/s eta 0:00:36\n",
      "   ------------ --------------------------- 43.3/136.5 MB 2.7 MB/s eta 0:00:36\n",
      "   ------------ --------------------------- 43.8/136.5 MB 2.7 MB/s eta 0:00:35\n",
      "   ------------ --------------------------- 44.0/136.5 MB 2.6 MB/s eta 0:00:36\n",
      "   ------------ --------------------------- 44.3/136.5 MB 2.6 MB/s eta 0:00:36\n",
      "   ------------- -------------------------- 44.8/136.5 MB 2.6 MB/s eta 0:00:36\n",
      "   ------------- -------------------------- 45.6/136.5 MB 2.6 MB/s eta 0:00:35\n",
      "   ------------- -------------------------- 45.6/136.5 MB 2.6 MB/s eta 0:00:35\n",
      "   ------------- -------------------------- 46.1/136.5 MB 2.6 MB/s eta 0:00:35\n",
      "   ------------- -------------------------- 46.9/136.5 MB 2.6 MB/s eta 0:00:35\n",
      "   ------------- -------------------------- 47.2/136.5 MB 2.6 MB/s eta 0:00:35\n",
      "   ------------- -------------------------- 47.7/136.5 MB 2.6 MB/s eta 0:00:35\n",
      "   -------------- ------------------------- 48.5/136.5 MB 2.6 MB/s eta 0:00:34\n",
      "   -------------- ------------------------- 49.3/136.5 MB 2.6 MB/s eta 0:00:34\n",
      "   -------------- ------------------------- 50.1/136.5 MB 2.6 MB/s eta 0:00:33\n",
      "   -------------- ------------------------- 50.6/136.5 MB 2.6 MB/s eta 0:00:33\n",
      "   -------------- ------------------------- 50.9/136.5 MB 2.6 MB/s eta 0:00:33\n",
      "   --------------- ------------------------ 51.6/136.5 MB 2.6 MB/s eta 0:00:33\n",
      "   --------------- ------------------------ 52.4/136.5 MB 2.6 MB/s eta 0:00:32\n",
      "   --------------- ------------------------ 53.2/136.5 MB 2.6 MB/s eta 0:00:32\n",
      "   --------------- ------------------------ 54.0/136.5 MB 2.7 MB/s eta 0:00:32\n",
      "   --------------- ------------------------ 54.5/136.5 MB 2.7 MB/s eta 0:00:31\n",
      "   ---------------- ----------------------- 55.3/136.5 MB 2.7 MB/s eta 0:00:31\n",
      "   ---------------- ----------------------- 55.8/136.5 MB 2.7 MB/s eta 0:00:31\n",
      "   ---------------- ----------------------- 56.4/136.5 MB 2.7 MB/s eta 0:00:30\n",
      "   ---------------- ----------------------- 56.6/136.5 MB 2.7 MB/s eta 0:00:30\n",
      "   ---------------- ----------------------- 57.1/136.5 MB 2.7 MB/s eta 0:00:30\n",
      "   ---------------- ----------------------- 57.9/136.5 MB 2.7 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 58.7/136.5 MB 2.7 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 59.2/136.5 MB 2.7 MB/s eta 0:00:29\n",
      "   ----------------- ---------------------- 59.8/136.5 MB 2.7 MB/s eta 0:00:29\n",
      "   ----------------- ---------------------- 60.6/136.5 MB 2.7 MB/s eta 0:00:29\n",
      "   ----------------- ---------------------- 60.8/136.5 MB 2.7 MB/s eta 0:00:29\n",
      "   ------------------ --------------------- 61.9/136.5 MB 2.7 MB/s eta 0:00:28\n",
      "   ------------------ --------------------- 62.7/136.5 MB 2.7 MB/s eta 0:00:28\n",
      "   ------------------ --------------------- 63.2/136.5 MB 2.7 MB/s eta 0:00:28\n",
      "   ------------------ --------------------- 64.0/136.5 MB 2.7 MB/s eta 0:00:27\n",
      "   ------------------ --------------------- 64.5/136.5 MB 2.7 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 65.3/136.5 MB 2.7 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 66.1/136.5 MB 2.7 MB/s eta 0:00:26\n",
      "   ------------------- -------------------- 67.1/136.5 MB 2.7 MB/s eta 0:00:26\n",
      "   ------------------- -------------------- 67.6/136.5 MB 2.7 MB/s eta 0:00:26\n",
      "   -------------------- ------------------- 68.4/136.5 MB 2.7 MB/s eta 0:00:25\n",
      "   -------------------- ------------------- 69.2/136.5 MB 2.7 MB/s eta 0:00:25\n",
      "   -------------------- ------------------- 69.5/136.5 MB 2.7 MB/s eta 0:00:25\n",
      "   -------------------- ------------------- 70.0/136.5 MB 2.7 MB/s eta 0:00:25\n",
      "   -------------------- ------------------- 70.8/136.5 MB 2.7 MB/s eta 0:00:24\n",
      "   -------------------- ------------------- 71.6/136.5 MB 2.8 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 72.4/136.5 MB 2.8 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 72.9/136.5 MB 2.8 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 73.4/136.5 MB 2.8 MB/s eta 0:00:23\n",
      "   --------------------- ------------------ 74.2/136.5 MB 2.8 MB/s eta 0:00:23\n",
      "   --------------------- ------------------ 74.4/136.5 MB 2.8 MB/s eta 0:00:23\n",
      "   ---------------------- ----------------- 75.2/136.5 MB 2.8 MB/s eta 0:00:23\n",
      "   ---------------------- ----------------- 75.8/136.5 MB 2.8 MB/s eta 0:00:23\n",
      "   ---------------------- ----------------- 76.3/136.5 MB 2.7 MB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 76.8/136.5 MB 2.7 MB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 77.3/136.5 MB 2.7 MB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 77.9/136.5 MB 2.7 MB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 78.1/136.5 MB 2.7 MB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 78.4/136.5 MB 2.7 MB/s eta 0:00:22\n",
      "   ----------------------- ---------------- 78.9/136.5 MB 2.7 MB/s eta 0:00:22\n",
      "   ----------------------- ---------------- 79.4/136.5 MB 2.7 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 80.0/136.5 MB 2.7 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 80.5/136.5 MB 2.7 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 81.0/136.5 MB 2.7 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 81.5/136.5 MB 2.7 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 81.8/136.5 MB 2.7 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 82.6/136.5 MB 2.7 MB/s eta 0:00:20\n",
      "   ------------------------ --------------- 83.1/136.5 MB 2.7 MB/s eta 0:00:20\n",
      "   ------------------------ --------------- 83.4/136.5 MB 2.7 MB/s eta 0:00:20\n",
      "   ------------------------ --------------- 83.9/136.5 MB 2.7 MB/s eta 0:00:20\n",
      "   ------------------------ --------------- 84.4/136.5 MB 2.7 MB/s eta 0:00:20\n",
      "   ------------------------ --------------- 85.2/136.5 MB 2.7 MB/s eta 0:00:19\n",
      "   ------------------------- -------------- 85.7/136.5 MB 2.7 MB/s eta 0:00:19\n",
      "   ------------------------- -------------- 86.5/136.5 MB 2.7 MB/s eta 0:00:19\n",
      "   ------------------------- -------------- 87.3/136.5 MB 2.7 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 88.1/136.5 MB 2.8 MB/s eta 0:00:18\n",
      "   -------------------------- ------------- 88.9/136.5 MB 2.8 MB/s eta 0:00:18\n",
      "   -------------------------- ------------- 89.7/136.5 MB 2.8 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 90.4/136.5 MB 2.8 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 91.0/136.5 MB 2.8 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 91.8/136.5 MB 2.8 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 92.0/136.5 MB 2.8 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 92.8/136.5 MB 2.8 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 93.3/136.5 MB 2.8 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 93.6/136.5 MB 2.8 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 93.8/136.5 MB 2.8 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 94.4/136.5 MB 2.8 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 94.6/136.5 MB 2.8 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 95.4/136.5 MB 2.8 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 95.7/136.5 MB 2.8 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 96.2/136.5 MB 2.8 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 96.7/136.5 MB 2.8 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 97.3/136.5 MB 2.8 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 97.8/136.5 MB 2.8 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 98.6/136.5 MB 2.8 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 99.1/136.5 MB 2.8 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 99.4/136.5 MB 2.8 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 99.9/136.5 MB 2.8 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 100.7/136.5 MB 2.8 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 101.2/136.5 MB 2.8 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 101.4/136.5 MB 2.8 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 102.0/136.5 MB 2.8 MB/s eta 0:00:13\n",
      "   ------------------------------ --------- 103.0/136.5 MB 2.8 MB/s eta 0:00:13\n",
      "   ------------------------------ --------- 103.5/136.5 MB 2.8 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 103.8/136.5 MB 2.7 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 104.6/136.5 MB 2.7 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 105.1/136.5 MB 2.8 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 105.6/136.5 MB 2.7 MB/s eta 0:00:12\n",
      "   ------------------------------- -------- 106.2/136.5 MB 2.7 MB/s eta 0:00:12\n",
      "   ------------------------------- -------- 106.7/136.5 MB 2.7 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 107.0/136.5 MB 2.7 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 107.7/136.5 MB 2.7 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 108.5/136.5 MB 2.7 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 109.1/136.5 MB 2.7 MB/s eta 0:00:11\n",
      "   -------------------------------- ------- 109.6/136.5 MB 2.7 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 110.4/136.5 MB 2.7 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 111.1/136.5 MB 2.7 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 111.4/136.5 MB 2.7 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 111.9/136.5 MB 2.7 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 112.5/136.5 MB 2.7 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 113.2/136.5 MB 2.7 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 113.8/136.5 MB 2.7 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 114.3/136.5 MB 2.7 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 115.1/136.5 MB 2.7 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 115.9/136.5 MB 2.7 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 116.7/136.5 MB 2.7 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 117.7/136.5 MB 2.7 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 118.5/136.5 MB 2.7 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 118.8/136.5 MB 2.7 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 119.5/136.5 MB 2.8 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 120.1/136.5 MB 2.7 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 120.8/136.5 MB 2.8 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 121.6/136.5 MB 2.8 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 121.9/136.5 MB 2.8 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 122.7/136.5 MB 2.8 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 123.2/136.5 MB 2.8 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 123.7/136.5 MB 2.8 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 124.5/136.5 MB 2.8 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 125.0/136.5 MB 2.8 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 125.3/136.5 MB 2.8 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 125.6/136.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 126.4/136.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 126.9/136.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 127.7/136.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 127.9/136.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 128.7/136.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 129.2/136.5 MB 2.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 130.0/136.5 MB 2.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 130.5/136.5 MB 2.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 131.3/136.5 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 132.1/136.5 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 132.6/136.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  133.2/136.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  133.7/136.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  134.2/136.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  135.0/136.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  135.8/136.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  136.3/136.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  136.3/136.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 136.5/136.5 MB 2.8 MB/s eta 0:00:00\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.43.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\ranad\\anaconda3\\lib\\site-packages (24.2)\n",
      "Requirement already satisfied: langchain in c:\\users\\ranad\\anaconda3\\lib\\site-packages (0.2.12)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (0.2.29)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (0.1.98)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (2.5.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (4.9.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.14.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain) (2.1)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain_community) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain_community) (3.8.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.12 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain_community) (0.2.12)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain_community) (0.2.29)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain_community) (0.1.98)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain_community) (1.25.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain_community) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain_community) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain<0.3.0,>=0.2.12->langchain_community) (0.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain<0.3.0,>=0.2.12->langchain_community) (2.5.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain_community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain_community) (4.9.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (2.0.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain_community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.12->langchain_community) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.12->langchain_community) (2.14.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Downloading langchain_community-0.2.11-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.3 MB 985.5 kB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.5/2.3 MB 985.5 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.8/2.3 MB 729.2 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.0/2.3 MB 967.3 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.6/2.3 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: typing-inspect, marshmallow, dataclasses-json, langchain_community\n",
      "Successfully installed dataclasses-json-0.6.7 langchain_community-0.2.11 marshmallow-3.21.3 typing-inspect-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'grep' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'grep' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.24.5)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.2.29)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface)\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tokenizers>=0.19.1 (from langchain-huggingface)\n",
      "  Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting transformers>=4.39.0 (from langchain-huggingface)\n",
      "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.1.98)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.5.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (8.2.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.25.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.11.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain-huggingface) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.14.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2023.11.17)\n",
      "Requirement already satisfied: sympy in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain-huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ranad\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
      "Downloading langchain_huggingface-0.0.3-py3-none-any.whl (17 kB)\n",
      "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.0/2.2 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.4 MB/s eta 0:00:00\n",
      "Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/9.5 MB 1.6 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 1.6/9.5 MB 2.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.4/9.5 MB 2.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.9/9.5 MB 2.8 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.4/9.5 MB 2.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.9/9.5 MB 2.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.2/9.5 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.7/9.5 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.8/9.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.0/9.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.8/9.5 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.6/9.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.1/9.5 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.7/9.5 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.2/9.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.5/9.5 MB 2.7 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, transformers, sentence-transformers, langchain-huggingface\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 2.1.1\n",
      "    Uninstalling transformers-2.1.1:\n",
      "      Successfully uninstalled transformers-2.1.1\n",
      "Successfully installed langchain-huggingface-0.0.3 sentence-transformers-3.0.1 tokenizers-0.19.1 transformers-4.44.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain\n",
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "# !pip install bitsandbytes\n",
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade langchain\n",
    "# !pip install langchain_community\n",
    "# !pip list | grep langchain\n",
    "# !pip list | grep langchain_community\n",
    "\n",
    "# !pip install -U langchain-huggingface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the pipeline with the langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T11:18:28.944831Z",
     "iopub.status.busy": "2024-07-12T11:18:28.944018Z",
     "iopub.status.idle": "2024-07-12T11:20:28.385866Z",
     "shell.execute_reply": "2024-07-12T11:20:28.385117Z",
     "shell.execute_reply.started": "2024-07-12T11:18:28.944793Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca299f3601f846499256f0fe028122af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import langchain\n",
    "\n",
    "### prompts\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "### models\n",
    "# from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "\n",
    "#model = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n",
    "model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "        \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T10:11:27.172667Z",
     "iopub.status.busy": "2024-07-12T10:11:27.171486Z",
     "iopub.status.idle": "2024-07-12T10:11:27.176608Z",
     "shell.execute_reply": "2024-07-12T10:11:27.175821Z",
     "shell.execute_reply.started": "2024-07-12T10:11:27.172632Z"
    }
   },
   "outputs": [],
   "source": [
    "question = questions_data[0]['question']\n",
    "options = \"\\nA. Ampicillin\\nB. Ceftriaxone\\nC. Doxycycline\\nD. Nitrofurantoin\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt design without a context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T11:21:04.821732Z",
     "iopub.status.busy": "2024-07-12T11:21:04.820863Z",
     "iopub.status.idle": "2024-07-12T11:21:04.826615Z",
     "shell.execute_reply": "2024-07-12T11:21:04.825723Z",
     "shell.execute_reply.started": "2024-07-12T11:21:04.821697Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "templateX = f\"\"\"Question: {question}[INST]Select the correct option only. No explanation required[/INST]\n",
    "\n",
    "Options:{options}\n",
    "\n",
    "#Answer:\"\"\"  # Force a single-line response\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(template=templateX, input_variables=[\"question\", \"options\"])\n",
    "promptX = prompt_template.format(question=question, options=options) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T11:21:32.443106Z",
     "iopub.status.busy": "2024-07-12T11:21:32.442313Z",
     "iopub.status.idle": "2024-07-12T11:21:33.646568Z",
     "shell.execute_reply": "2024-07-12T11:21:33.645670Z",
     "shell.execute_reply.started": "2024-07-12T11:21:32.443076Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?[INST]Select the correct option only. No explanation required[/INST]\n",
      "\n",
      "Options:\n",
      "A. Ampicillin\n",
      "B. Ceftriaxone\n",
      "C. Doxycycline\n",
      "D. Nitrofurantoin\n",
      "\n",
      "\n",
      "#Answer: D\n"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "inputs = tokenizer(promptX, return_tensors='pt', truncation=True, padding=\"max_length\", max_length=1024).to(model_llama.device)\n",
    "outputs = model_llama.generate(**inputs, max_new_tokens=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T11:21:23.118608Z",
     "iopub.status.busy": "2024-07-12T11:21:23.117944Z",
     "iopub.status.idle": "2024-07-12T11:21:23.124773Z",
     "shell.execute_reply": "2024-07-12T11:21:23.123737Z",
     "shell.execute_reply.started": "2024-07-12T11:21:23.118576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = response.find('#Answer:')\n",
    "prediction = response[position+8 :position+10].strip()\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt generation\n",
    "template_context = f\"\"\"Question: {question}\n",
    "Context: {context}[INST]Select the correct option only. No explanation required[/INST]\n",
    "\n",
    "Options: {options}\n",
    "\n",
    "# Answer: \"\"\" # Force a single-line response\n",
    "\n",
    "prompt_context = PromptTemplate(template=template_context, input_variables=[\"question\", \"options\", \"context\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "inputs = tokenizer(prompt_context, return_tensors='pt', truncation=True, padding=\"max_length\", max_length=4000).to(model_llama.device)\n",
    "outputs = model_llama.generate(**inputs, max_new_tokens=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = response.find('#Answer:')\n",
    "prediction = response[position+8 :position+10].strip()\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing a context summary prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the summarization prompt to generate a summary\n",
    "summarization_prompt = f\"\"\"[INST] Summarize the following text concisely:\n",
    "\n",
    "{context_text}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    summary_output = model_llama.generate(\n",
    "        **tokenizer(summarization_prompt, return_tensors=\"pt\").to(model_llama.device),\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "\n",
    "# Decode and print the generated summary\n",
    "summary_text = tokenizer.decode(summary_output[0], skip_special_tokens=True)\n",
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing the data loader for parsing the data faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T11:21:43.697486Z",
     "iopub.status.busy": "2024-07-12T11:21:43.696672Z",
     "iopub.status.idle": "2024-07-12T11:21:43.705272Z",
     "shell.execute_reply": "2024-07-12T11:21:43.704397Z",
     "shell.execute_reply.started": "2024-07-12T11:21:43.697451Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "class QuestionAnswerDataset(Dataset):\n",
    "    def __init__(self, questions_data):\n",
    "        self.questionData = questions_data\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questionData)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question_data = self.questionData[idx]\n",
    "        question = question_data['question']\n",
    "        options = question_data['options']\n",
    "        options_str = \"\\n\".join([f\"{key}. {value}\" for key, value in options.items()])\n",
    "        answer = question_data['answer_idx']\n",
    "        return question,options_str,answer\n",
    "\n",
    "# Load your dataset\n",
    "dataset = QuestionAnswerDataset(questions_data)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=False)  # Adjust batch_size as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader with RAG using FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnswerDataset_RAG(Dataset):\n",
    "    def __init__(self, questions_data, faiss_index, faiss_texts):\n",
    "        self.questionData = questions_data\n",
    "        self.faiss_index = faiss_index\n",
    "        self.faiss_texts = faiss_texts\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questionData)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question_data = self.questionData[idx]\n",
    "        question = question_data['question']\n",
    "        options = question_data['options']\n",
    "        options_str = \"\\n\".join([f\"{key}. {value}\" for key, value in options.items()])\n",
    "        answer = question_data['answer_idx']\n",
    "\n",
    "        # Search FAISS index for each option and retrieve the corresponding text\n",
    "        retrieved_contexts = []\n",
    "        for option_key, option_text in options.items():\n",
    "            option_embedding = self.get_text_embedding(option_text)\n",
    "            retrieved_indices = search_faiss_index(option_embedding, self.faiss_index)\n",
    "            retrieved_context = \" \".join([self.faiss_texts[i] for i in retrieved_indices])\n",
    "            retrieved_contexts.append(retrieved_context)\n",
    "\n",
    "        # Combine the contexts with the question and options for the final return\n",
    "        combined_context = \"\\n\".join(retrieved_contexts)\n",
    "\n",
    "        return question, options_str, answer, combined_context\n",
    "\n",
    "    def get_text_embedding(self, text):\n",
    "        # Placeholder for the actual embedding generation logic\n",
    "        # Replace this with the method to generate embeddings from text\n",
    "        return np.random.rand(768)  # Example: Replace with actual embedding\n",
    "\n",
    "\n",
    "faiss_texts = [...]  # Load the list of texts associated with the FAISS index\n",
    "\n",
    "# Load your dataset\n",
    "dataset_RAG = QuestionAnswerDataset_RAG(questions_data, faiss_index, faiss_texts)\n",
    "dataloader_RAG = DataLoader(dataset, batch_size=8, shuffle=False)  # Adjust batch_size as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt design and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T11:21:48.441489Z",
     "iopub.status.busy": "2024-07-12T11:21:48.440805Z",
     "iopub.status.idle": "2024-07-12T11:21:48.446136Z",
     "shell.execute_reply": "2024-07-12T11:21:48.445150Z",
     "shell.execute_reply.started": "2024-07-12T11:21:48.441456Z"
    }
   },
   "outputs": [],
   "source": [
    "#prompt generation\n",
    "template = f\"\"\"Question: {question}[INST]Select the correct option only. No explanation required[/INST]\n",
    "\n",
    "Options:{options}\n",
    "\n",
    "#Answer:\"\"\"  # Force a single-line response\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"options\"])\n",
    "\n",
    "#llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-07-12T11:22:50.051026Z",
     "iopub.status.busy": "2024-07-12T11:22:50.050307Z",
     "iopub.status.idle": "2024-07-12T12:11:51.003758Z",
     "shell.execute_reply": "2024-07-12T12:11:51.002747Z",
     "shell.execute_reply.started": "2024-07-12T11:22:50.050994Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1273 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 1/1273 [00:02<44:09,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 2/1273 [00:04<43:52,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 3/1273 [00:06<44:00,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 4/1273 [00:08<43:29,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 5/1273 [00:10<44:52,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 6/1273 [00:13<48:50,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|          | 7/1273 [00:15<46:56,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|          | 8/1273 [00:17<46:16,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|          | 9/1273 [00:19<45:04,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|          | 10/1273 [00:21<44:59,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|          | 11/1273 [00:24<48:45,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|          | 12/1273 [00:26<50:19,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|          | 13/1273 [00:28<45:58,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|          | 14/1273 [00:30<46:54,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|          | 15/1273 [00:33<49:31,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|▏         | 16/1273 [00:35<47:52,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|▏         | 17/1273 [00:38<51:48,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|▏         | 18/1273 [00:41<53:30,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|▏         | 19/1273 [00:42<48:07,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 20/1273 [00:45<46:35,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 21/1273 [00:47<48:54,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 22/1273 [00:50<50:09,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 23/1273 [00:52<47:53,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 24/1273 [00:54<45:58,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 25/1273 [00:56<44:49,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 26/1273 [00:58<45:53,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 27/1273 [01:00<46:20,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 28/1273 [01:03<50:15,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 29/1273 [01:05<45:47,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 30/1273 [01:07<43:25,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 31/1273 [01:09<43:04,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 32/1273 [01:12<46:58,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 33/1273 [01:14<45:13,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 34/1273 [01:15<41:53,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 35/1273 [01:17<42:02,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 36/1273 [01:20<43:38,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 37/1273 [01:22<44:33,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 38/1273 [01:25<47:35,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 39/1273 [01:27<45:40,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 40/1273 [01:29<48:31,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 41/1273 [01:33<54:30,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 42/1273 [01:35<51:03,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 43/1273 [01:37<52:18,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 44/1273 [01:41<56:28,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▎         | 45/1273 [01:43<53:38,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▎         | 46/1273 [01:46<53:29,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▎         | 47/1273 [01:48<50:19,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 48/1273 [01:50<51:40,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 49/1273 [01:53<54:18,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 50/1273 [01:56<55:26,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 51/1273 [01:58<50:10,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 52/1273 [02:00<48:59,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 53/1273 [02:02<46:38,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 54/1273 [02:05<50:49,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 55/1273 [02:08<51:58,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 56/1273 [02:11<53:34,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 57/1273 [02:13<49:29,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▍         | 58/1273 [02:15<49:36,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▍         | 59/1273 [02:18<51:54,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▍         | 60/1273 [02:21<51:22,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▍         | 61/1273 [02:24<56:38,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▍         | 62/1273 [02:26<53:36,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▍         | 63/1273 [02:28<50:13,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▌         | 64/1273 [02:30<45:08,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▌         | 65/1273 [02:32<45:10,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▌         | 66/1273 [02:34<44:43,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▌         | 67/1273 [02:36<43:18,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▌         | 68/1273 [02:39<45:29,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▌         | 69/1273 [02:41<43:01,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▌         | 70/1273 [02:44<47:51,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▌         | 71/1273 [02:47<50:57,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▌         | 72/1273 [02:49<47:49,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▌         | 73/1273 [02:51<47:22,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▌         | 74/1273 [02:53<44:19,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▌         | 75/1273 [02:56<47:52,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▌         | 76/1273 [02:58<44:22,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▌         | 77/1273 [03:00<43:41,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▌         | 78/1273 [03:02<44:08,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▌         | 79/1273 [03:05<47:09,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▋         | 80/1273 [03:07<49:15,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▋         | 81/1273 [03:09<45:25,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▋         | 82/1273 [03:11<44:15,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 83/1273 [03:13<43:23,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 84/1273 [03:16<46:07,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 85/1273 [03:18<45:39,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 86/1273 [03:21<45:37,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 87/1273 [03:23<43:58,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 88/1273 [03:25<41:56,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 89/1273 [03:27<41:15,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 90/1273 [03:29<42:24,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 91/1273 [03:31<43:20,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 92/1273 [03:33<43:44,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 93/1273 [03:36<46:51,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 94/1273 [03:39<50:22,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 95/1273 [03:41<46:04,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 96/1273 [03:43<43:52,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 97/1273 [03:47<54:48,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 98/1273 [03:49<49:10,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 99/1273 [03:51<46:12,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 100/1273 [03:53<46:46,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 101/1273 [03:56<48:31,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 102/1273 [03:59<47:48,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 103/1273 [04:02<53:04,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 104/1273 [04:04<48:52,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 105/1273 [04:06<45:51,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 106/1273 [04:08<46:31,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 107/1273 [04:12<51:17,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 108/1273 [04:13<46:47,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▊         | 109/1273 [04:16<48:14,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▊         | 110/1273 [04:18<45:17,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▊         | 111/1273 [04:20<42:29,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▉         | 112/1273 [04:22<40:36,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▉         | 113/1273 [04:24<41:46,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▉         | 114/1273 [04:27<46:21,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▉         | 115/1273 [04:29<43:56,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▉         | 116/1273 [04:31<41:39,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▉         | 117/1273 [04:33<40:54,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▉         | 118/1273 [04:35<40:45,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▉         | 119/1273 [04:37<40:11,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▉         | 120/1273 [04:39<38:46,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|▉         | 121/1273 [04:41<38:59,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|▉         | 122/1273 [04:44<41:54,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|▉         | 123/1273 [04:45<39:47,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|▉         | 124/1273 [04:48<41:02,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|▉         | 125/1273 [04:50<40:51,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|▉         | 126/1273 [04:53<43:49,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|▉         | 127/1273 [04:55<46:12,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|█         | 128/1273 [04:58<48:10,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|█         | 129/1273 [05:00<43:21,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|█         | 130/1273 [05:02<41:52,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|█         | 131/1273 [05:04<42:34,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|█         | 132/1273 [05:06<42:55,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|█         | 133/1273 [05:09<44:48,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█         | 134/1273 [05:11<41:43,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█         | 135/1273 [05:14<45:52,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█         | 136/1273 [05:16<43:27,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█         | 137/1273 [05:18<44:24,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█         | 138/1273 [05:20<43:52,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█         | 139/1273 [05:23<43:38,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█         | 140/1273 [05:25<40:44,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█         | 141/1273 [05:27<41:58,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█         | 142/1273 [05:29<41:01,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█         | 143/1273 [05:31<39:19,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█▏        | 144/1273 [05:33<38:50,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█▏        | 145/1273 [05:35<38:34,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█▏        | 146/1273 [05:37<38:15,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 147/1273 [05:39<39:40,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 148/1273 [05:41<40:09,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 149/1273 [05:44<41:52,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 150/1273 [05:46<39:09,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 151/1273 [05:48<40:09,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 152/1273 [05:50<40:27,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 153/1273 [05:52<40:54,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 154/1273 [05:54<39:47,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 155/1273 [05:56<37:58,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 156/1273 [05:58<39:15,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 157/1273 [06:00<38:35,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 158/1273 [06:03<41:42,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 159/1273 [06:05<42:02,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 160/1273 [06:07<40:32,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 161/1273 [06:10<43:06,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 162/1273 [06:13<48:45,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 163/1273 [06:16<46:35,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 164/1273 [06:17<42:11,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 165/1273 [06:20<44:56,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 166/1273 [06:23<45:30,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 167/1273 [06:25<44:30,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 168/1273 [06:27<41:19,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 169/1273 [06:29<42:11,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 170/1273 [06:31<39:32,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 171/1273 [06:33<39:05,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▎        | 172/1273 [06:36<41:09,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▎        | 173/1273 [06:38<42:23,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▎        | 174/1273 [06:41<43:36,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▎        | 175/1273 [06:43<41:28,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 176/1273 [06:46<44:59,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 177/1273 [06:48<42:37,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 178/1273 [06:50<42:23,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 179/1273 [06:53<45:07,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 180/1273 [06:55<45:05,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 181/1273 [06:59<51:41,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 182/1273 [07:01<47:25,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 183/1273 [07:03<45:29,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 184/1273 [07:05<39:19,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▍        | 185/1273 [07:07<40:30,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▍        | 186/1273 [07:09<39:12,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▍        | 187/1273 [07:11<37:09,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▍        | 188/1273 [07:13<36:51,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▍        | 189/1273 [07:15<40:08,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▍        | 190/1273 [07:17<38:49,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▌        | 191/1273 [07:19<37:58,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▌        | 192/1273 [07:21<37:10,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▌        | 193/1273 [07:24<39:19,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▌        | 194/1273 [07:26<40:11,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▌        | 195/1273 [07:29<42:38,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▌        | 196/1273 [07:31<40:53,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▌        | 197/1273 [07:33<41:08,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▌        | 198/1273 [07:36<43:09,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▌        | 199/1273 [07:38<43:26,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▌        | 200/1273 [07:42<47:24,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▌        | 201/1273 [07:44<46:45,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▌        | 202/1273 [07:47<47:21,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▌        | 203/1273 [07:50<51:30,  2.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▌        | 204/1273 [07:53<49:18,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▌        | 205/1273 [07:55<45:37,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▌        | 206/1273 [07:57<45:10,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▋        | 207/1273 [07:59<42:14,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▋        | 208/1273 [08:02<42:45,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▋        | 209/1273 [08:04<42:00,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▋        | 210/1273 [08:06<41:47,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 211/1273 [08:09<44:04,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 212/1273 [08:11<41:27,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 213/1273 [08:14<40:57,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 214/1273 [08:16<41:46,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 215/1273 [08:18<38:18,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 216/1273 [08:20<37:27,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 217/1273 [08:22<36:41,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 218/1273 [08:24<36:16,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 219/1273 [08:26<37:34,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 220/1273 [08:30<44:48,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 221/1273 [08:32<45:14,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 222/1273 [08:35<45:03,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 223/1273 [08:37<42:27,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 224/1273 [08:39<42:39,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 225/1273 [08:43<46:23,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 226/1273 [08:45<44:18,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 227/1273 [08:48<46:05,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 228/1273 [08:50<41:52,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 229/1273 [08:51<38:50,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 230/1273 [08:55<44:28,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 231/1273 [08:57<41:27,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 232/1273 [08:59<39:31,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 233/1273 [09:01<40:33,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 234/1273 [09:03<38:50,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 235/1273 [09:05<38:43,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▊        | 236/1273 [09:08<39:52,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▊        | 237/1273 [09:11<44:22,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▊        | 238/1273 [09:13<41:14,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 239/1273 [09:15<37:37,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 240/1273 [09:17<35:48,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 241/1273 [09:19<35:47,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 242/1273 [09:21<34:41,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 243/1273 [09:23<39:18,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 244/1273 [09:25<37:15,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 245/1273 [09:27<35:34,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 246/1273 [09:30<39:02,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 247/1273 [09:32<37:42,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 248/1273 [09:34<38:03,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|█▉        | 249/1273 [09:36<37:09,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|█▉        | 250/1273 [09:39<37:31,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|█▉        | 251/1273 [09:41<38:51,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|█▉        | 252/1273 [09:44<40:59,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|█▉        | 253/1273 [09:46<41:19,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|█▉        | 254/1273 [09:49<40:51,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|██        | 255/1273 [09:51<39:49,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|██        | 256/1273 [09:54<45:34,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|██        | 257/1273 [09:56<40:43,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|██        | 258/1273 [09:58<38:52,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|██        | 259/1273 [10:00<37:49,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|██        | 260/1273 [10:03<41:06,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 261/1273 [10:06<41:24,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 262/1273 [10:08<40:24,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 263/1273 [10:10<38:44,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 264/1273 [10:13<41:01,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 265/1273 [10:15<38:49,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 266/1273 [10:17<37:06,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 267/1273 [10:19<35:24,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 268/1273 [10:21<35:10,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 269/1273 [10:23<36:01,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 270/1273 [10:25<35:16,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██▏       | 271/1273 [10:27<34:02,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██▏       | 272/1273 [10:29<36:02,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██▏       | 273/1273 [10:31<35:33,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 274/1273 [10:34<36:22,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 275/1273 [10:36<36:53,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 276/1273 [10:39<39:37,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 277/1273 [10:41<37:13,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 278/1273 [10:43<38:20,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 279/1273 [10:45<36:49,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 280/1273 [10:48<38:20,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 281/1273 [10:50<39:28,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 282/1273 [10:53<43:10,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 283/1273 [10:57<48:28,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 284/1273 [10:59<44:11,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 285/1273 [11:01<41:14,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 286/1273 [11:04<42:41,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 287/1273 [11:06<40:40,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 288/1273 [11:09<42:04,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 289/1273 [11:12<42:20,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 290/1273 [11:14<42:13,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 291/1273 [11:16<40:48,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 292/1273 [11:19<39:49,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 293/1273 [11:21<39:41,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 294/1273 [11:23<36:53,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 295/1273 [11:25<35:25,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 296/1273 [11:27<34:25,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 297/1273 [11:29<34:08,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 298/1273 [11:31<33:31,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 299/1273 [11:33<35:36,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▎       | 300/1273 [11:35<33:55,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▎       | 301/1273 [11:38<36:33,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▎       | 302/1273 [11:41<39:12,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 303/1273 [11:43<36:20,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 304/1273 [11:44<33:38,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 305/1273 [11:47<36:56,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 306/1273 [11:49<33:53,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 307/1273 [11:51<32:18,  2.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 308/1273 [11:52<31:26,  1.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 309/1273 [11:54<30:44,  1.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 310/1273 [11:57<32:37,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 311/1273 [11:59<32:53,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▍       | 312/1273 [12:00<32:03,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▍       | 313/1273 [12:03<34:39,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▍       | 314/1273 [12:05<33:16,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▍       | 315/1273 [12:08<36:15,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▍       | 316/1273 [12:10<35:25,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▍       | 317/1273 [12:12<33:43,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▍       | 318/1273 [12:13<32:23,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▌       | 319/1273 [12:16<32:42,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▌       | 320/1273 [12:19<40:26,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▌       | 321/1273 [12:22<39:03,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▌       | 322/1273 [12:24<38:23,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▌       | 323/1273 [12:26<38:17,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▌       | 324/1273 [12:29<37:33,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▌       | 325/1273 [12:31<37:53,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▌       | 326/1273 [12:33<35:19,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▌       | 327/1273 [12:35<33:26,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▌       | 328/1273 [12:37<32:13,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▌       | 329/1273 [12:39<32:02,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▌       | 330/1273 [12:41<35:30,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▌       | 331/1273 [12:44<37:49,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▌       | 332/1273 [12:46<37:08,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▌       | 333/1273 [12:48<35:38,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▌       | 334/1273 [12:50<34:26,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▋       | 335/1273 [12:52<32:41,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▋       | 336/1273 [12:54<32:13,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▋       | 337/1273 [12:56<32:15,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 338/1273 [12:58<31:10,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 339/1273 [13:00<31:09,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 340/1273 [13:02<31:32,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 341/1273 [13:05<33:45,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 342/1273 [13:07<33:24,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 343/1273 [13:10<36:17,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 344/1273 [13:12<36:40,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 345/1273 [13:14<33:25,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 346/1273 [13:17<37:55,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 347/1273 [13:19<37:00,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 348/1273 [13:22<36:22,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 349/1273 [13:24<36:49,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 350/1273 [13:27<39:30,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 351/1273 [13:29<36:59,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 352/1273 [13:31<36:19,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 353/1273 [13:33<33:46,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 354/1273 [13:35<32:48,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 355/1273 [13:38<35:33,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 356/1273 [13:40<34:23,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 357/1273 [13:43<36:52,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 358/1273 [13:45<37:19,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 359/1273 [13:48<38:43,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 360/1273 [13:51<39:04,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 361/1273 [13:54<41:47,  2.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 362/1273 [13:57<42:03,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▊       | 363/1273 [13:59<39:56,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▊       | 364/1273 [14:01<39:10,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▊       | 365/1273 [14:04<37:35,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▉       | 366/1273 [14:06<35:19,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▉       | 367/1273 [14:08<34:57,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▉       | 368/1273 [14:11<39:39,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▉       | 369/1273 [14:14<37:59,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▉       | 370/1273 [14:16<37:44,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▉       | 371/1273 [14:19<37:39,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▉       | 372/1273 [14:21<37:44,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▉       | 373/1273 [14:23<34:06,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▉       | 374/1273 [14:25<32:56,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▉       | 375/1273 [14:27<32:07,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|██▉       | 376/1273 [14:29<32:37,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|██▉       | 377/1273 [14:31<31:44,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|██▉       | 378/1273 [14:34<33:11,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|██▉       | 379/1273 [14:36<32:16,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|██▉       | 380/1273 [14:38<31:52,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|██▉       | 381/1273 [14:40<33:31,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|███       | 382/1273 [14:43<34:40,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|███       | 383/1273 [14:45<35:21,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|███       | 384/1273 [14:47<33:35,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|███       | 385/1273 [14:49<32:50,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|███       | 386/1273 [14:52<33:02,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|███       | 387/1273 [14:54<33:33,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|███       | 388/1273 [14:56<33:37,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 389/1273 [14:58<33:32,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 390/1273 [15:01<35:50,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 391/1273 [15:03<32:38,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 392/1273 [15:05<31:42,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 393/1273 [15:07<30:12,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 394/1273 [15:09<29:56,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 395/1273 [15:11<29:44,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 396/1273 [15:13<28:42,  1.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 397/1273 [15:16<33:44,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███▏      | 398/1273 [15:18<33:39,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███▏      | 399/1273 [15:20<33:21,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███▏      | 400/1273 [15:24<37:21,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 401/1273 [15:26<34:46,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 402/1273 [15:28<36:07,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 403/1273 [15:30<33:15,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 404/1273 [15:32<32:18,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 405/1273 [15:34<32:37,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 406/1273 [15:36<31:27,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 407/1273 [15:39<33:40,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 408/1273 [15:41<33:21,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 409/1273 [15:44<32:11,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 410/1273 [15:47<37:37,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 411/1273 [15:49<35:21,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 412/1273 [15:51<33:12,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 413/1273 [15:54<34:00,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 414/1273 [15:56<35:37,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 415/1273 [15:58<32:08,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 416/1273 [16:01<33:17,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 417/1273 [16:03<33:13,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 418/1273 [16:06<34:48,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 419/1273 [16:08<35:00,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 420/1273 [16:10<32:20,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 421/1273 [16:13<34:01,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 422/1273 [16:16<37:17,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 423/1273 [16:19<38:10,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 424/1273 [16:21<38:33,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 425/1273 [16:23<35:30,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 426/1273 [16:26<35:33,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▎      | 427/1273 [16:28<35:06,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▎      | 428/1273 [16:31<35:41,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▎      | 429/1273 [16:33<33:22,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 430/1273 [16:36<34:10,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 431/1273 [16:38<34:20,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 432/1273 [16:41<34:15,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 433/1273 [16:42<30:57,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 434/1273 [16:44<30:00,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 435/1273 [16:46<29:23,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 436/1273 [16:49<32:26,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 437/1273 [16:52<34:19,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 438/1273 [16:54<34:32,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 439/1273 [16:57<33:47,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▍      | 440/1273 [16:58<31:08,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▍      | 441/1273 [17:00<29:11,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▍      | 442/1273 [17:03<30:49,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▍      | 443/1273 [17:05<30:09,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▍      | 444/1273 [17:07<29:24,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▍      | 445/1273 [17:10<32:02,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▌      | 446/1273 [17:12<30:41,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▌      | 447/1273 [17:14<29:43,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▌      | 448/1273 [17:16<30:10,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▌      | 449/1273 [17:18<31:30,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▌      | 450/1273 [17:21<32:11,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▌      | 451/1273 [17:23<30:47,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▌      | 452/1273 [17:25<32:07,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▌      | 453/1273 [17:28<31:47,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▌      | 454/1273 [17:30<30:46,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▌      | 455/1273 [17:32<29:15,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▌      | 456/1273 [17:34<29:43,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▌      | 457/1273 [17:37<33:33,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▌      | 458/1273 [17:39<32:40,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▌      | 459/1273 [17:42<34:13,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▌      | 460/1273 [17:44<32:08,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▌      | 461/1273 [17:46<29:58,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▋      | 462/1273 [17:49<32:38,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▋      | 463/1273 [17:51<31:57,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▋      | 464/1273 [17:53<30:25,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 465/1273 [17:55<30:27,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 466/1273 [17:58<30:24,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 467/1273 [18:00<29:26,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 468/1273 [18:02<30:28,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 469/1273 [18:05<31:21,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 470/1273 [18:08<35:41,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 471/1273 [18:11<37:32,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 472/1273 [18:13<33:39,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 473/1273 [18:15<32:29,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 474/1273 [18:18<34:16,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 475/1273 [18:21<34:36,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 476/1273 [18:24<34:38,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 477/1273 [18:26<32:12,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 478/1273 [18:28<31:35,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 479/1273 [18:30<30:19,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 480/1273 [18:32<30:19,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 481/1273 [18:36<36:22,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 482/1273 [18:39<37:48,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 483/1273 [18:41<34:21,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 484/1273 [18:43<32:18,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 485/1273 [18:45<30:28,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 486/1273 [18:47<29:24,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 487/1273 [18:50<29:29,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 488/1273 [18:52<30:19,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 489/1273 [18:54<29:03,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 490/1273 [18:56<28:14,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▊      | 491/1273 [18:59<29:25,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▊      | 492/1273 [19:01<29:17,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▊      | 493/1273 [19:03<30:48,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▉      | 494/1273 [19:06<32:17,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▉      | 495/1273 [19:09<32:16,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▉      | 496/1273 [19:11<32:09,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▉      | 497/1273 [19:14<32:02,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▉      | 498/1273 [19:16<31:16,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▉      | 499/1273 [19:18<29:44,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▉      | 500/1273 [19:20<30:12,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▉      | 501/1273 [19:23<30:04,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▉      | 502/1273 [19:25<30:30,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|███▉      | 503/1273 [19:27<28:23,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|███▉      | 504/1273 [19:29<28:32,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|███▉      | 505/1273 [19:31<27:51,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|███▉      | 506/1273 [19:34<28:31,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|███▉      | 507/1273 [19:36<27:56,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|███▉      | 508/1273 [19:38<27:18,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|███▉      | 509/1273 [19:40<28:16,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████      | 510/1273 [19:42<26:14,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████      | 511/1273 [19:45<29:46,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████      | 512/1273 [19:48<30:39,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████      | 513/1273 [19:50<29:06,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████      | 514/1273 [19:52<28:09,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████      | 515/1273 [19:54<27:35,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████      | 516/1273 [19:56<28:35,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████      | 517/1273 [19:58<27:40,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████      | 518/1273 [20:00<26:52,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████      | 519/1273 [20:03<28:23,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████      | 520/1273 [20:05<27:42,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████      | 521/1273 [20:07<26:06,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████      | 522/1273 [20:09<26:05,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████      | 523/1273 [20:11<27:29,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████      | 524/1273 [20:14<28:30,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████      | 525/1273 [20:16<28:41,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████▏     | 526/1273 [20:18<27:49,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████▏     | 527/1273 [20:20<26:57,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████▏     | 528/1273 [20:22<26:33,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 529/1273 [20:24<26:07,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 530/1273 [20:27<28:31,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 531/1273 [20:29<29:02,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 532/1273 [20:31<27:51,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 533/1273 [20:34<28:44,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 534/1273 [20:36<28:59,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 535/1273 [20:39<31:10,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 536/1273 [20:41<28:38,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 537/1273 [20:43<26:08,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 538/1273 [20:45<25:36,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 539/1273 [20:47<25:10,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 540/1273 [20:49<24:59,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 541/1273 [20:52<27:29,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 542/1273 [20:54<29:27,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 543/1273 [20:57<29:04,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 544/1273 [20:59<28:34,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 545/1273 [21:01<27:16,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 546/1273 [21:04<31:24,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 547/1273 [21:08<35:20,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 548/1273 [21:10<32:19,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 549/1273 [21:13<32:34,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 550/1273 [21:16<33:17,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 551/1273 [21:18<31:23,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 552/1273 [21:21<31:55,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 553/1273 [21:23<31:21,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▎     | 554/1273 [21:25<28:42,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▎     | 555/1273 [21:29<32:36,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▎     | 556/1273 [21:31<29:54,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 557/1273 [21:34<33:57,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 558/1273 [21:36<30:18,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 559/1273 [21:38<28:34,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 560/1273 [21:41<30:24,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 561/1273 [21:43<27:44,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 562/1273 [21:45<26:28,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 563/1273 [21:47<26:23,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 564/1273 [21:50<28:23,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 565/1273 [21:52<26:26,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 566/1273 [21:54<25:53,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▍     | 567/1273 [21:56<25:27,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▍     | 568/1273 [21:58<25:07,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▍     | 569/1273 [22:00<24:46,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▍     | 570/1273 [22:03<25:25,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▍     | 571/1273 [22:05<25:09,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▍     | 572/1273 [22:07<27:22,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▌     | 573/1273 [22:10<27:54,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▌     | 574/1273 [22:12<28:04,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▌     | 575/1273 [22:14<25:49,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▌     | 576/1273 [22:16<25:00,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▌     | 577/1273 [22:18<24:42,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▌     | 578/1273 [22:21<27:25,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▌     | 579/1273 [22:24<28:21,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 580/1273 [22:26<26:46,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 581/1273 [22:28<26:45,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 582/1273 [22:30<25:50,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 583/1273 [22:33<26:46,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 584/1273 [22:35<26:29,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 585/1273 [22:38<27:58,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 586/1273 [22:40<26:42,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 587/1273 [22:42<27:17,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 588/1273 [22:44<26:12,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▋     | 589/1273 [22:46<25:25,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▋     | 590/1273 [22:48<24:38,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▋     | 591/1273 [22:50<24:01,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 592/1273 [22:53<25:06,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 593/1273 [22:55<24:16,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 594/1273 [22:57<23:46,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 595/1273 [22:59<22:58,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 596/1273 [23:01<22:19,  1.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 597/1273 [23:03<23:50,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 598/1273 [23:05<24:18,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 599/1273 [23:07<22:37,  2.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 600/1273 [23:09<22:29,  2.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 601/1273 [23:12<25:31,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 602/1273 [23:14<24:53,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 603/1273 [23:16<24:03,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 604/1273 [23:18<24:27,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 605/1273 [23:21<24:45,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 606/1273 [23:23<26:30,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 607/1273 [23:25<25:24,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 608/1273 [23:28<26:20,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 609/1273 [23:30<26:37,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 610/1273 [23:32<25:08,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 611/1273 [23:34<24:12,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 612/1273 [23:37<24:28,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 613/1273 [23:39<24:36,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 614/1273 [23:42<26:03,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 615/1273 [23:45<28:03,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 616/1273 [23:48<29:31,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 617/1273 [23:50<27:25,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▊     | 618/1273 [23:52<26:34,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▊     | 619/1273 [23:54<25:05,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▊     | 620/1273 [23:57<26:26,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▉     | 621/1273 [23:59<24:59,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▉     | 622/1273 [24:02<26:38,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▉     | 623/1273 [24:04<26:38,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▉     | 624/1273 [24:07<28:22,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▉     | 625/1273 [24:10<27:49,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▉     | 626/1273 [24:12<27:32,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▉     | 627/1273 [24:14<25:59,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▉     | 628/1273 [24:16<24:34,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▉     | 629/1273 [24:18<24:34,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▉     | 630/1273 [24:21<24:27,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|████▉     | 631/1273 [24:23<23:28,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|████▉     | 632/1273 [24:25<24:31,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|████▉     | 633/1273 [24:27<23:36,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|████▉     | 634/1273 [24:29<22:57,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|████▉     | 635/1273 [24:31<22:21,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|████▉     | 636/1273 [24:33<22:53,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 637/1273 [24:36<23:49,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 638/1273 [24:38<23:07,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 639/1273 [24:40<21:48,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 640/1273 [24:42<21:44,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 641/1273 [24:44<22:23,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 642/1273 [24:46<21:49,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████     | 643/1273 [24:48<20:57,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████     | 644/1273 [24:51<23:27,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████     | 645/1273 [24:53<24:12,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████     | 646/1273 [24:56<24:40,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████     | 647/1273 [24:57<22:30,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████     | 648/1273 [24:59<21:56,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████     | 649/1273 [25:01<21:52,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████     | 650/1273 [25:04<22:53,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████     | 651/1273 [25:07<26:20,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████     | 652/1273 [25:10<26:02,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████▏    | 653/1273 [25:12<24:27,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████▏    | 654/1273 [25:14<23:18,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████▏    | 655/1273 [25:16<25:04,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 656/1273 [25:19<23:48,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 657/1273 [25:20<22:09,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 658/1273 [25:23<23:04,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 659/1273 [25:25<22:14,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 660/1273 [25:27<22:37,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 661/1273 [25:30<23:29,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 662/1273 [25:32<22:42,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 663/1273 [25:34<24:06,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 664/1273 [25:36<22:57,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 665/1273 [25:39<24:43,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 666/1273 [25:42<24:53,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 667/1273 [25:44<22:51,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 668/1273 [25:45<21:34,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 669/1273 [25:47<20:35,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 670/1273 [25:49<20:23,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 671/1273 [25:52<22:32,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 672/1273 [25:54<22:29,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 673/1273 [25:56<20:55,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 674/1273 [25:58<22:11,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 675/1273 [26:00<21:25,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 676/1273 [26:02<20:26,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 677/1273 [26:04<19:12,  1.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 678/1273 [26:07<22:05,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 679/1273 [26:09<23:19,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 680/1273 [26:12<22:24,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 681/1273 [26:14<22:31,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▎    | 682/1273 [26:16<21:50,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▎    | 683/1273 [26:18<21:58,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▎    | 684/1273 [26:20<21:27,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▍    | 685/1273 [26:23<23:43,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▍    | 686/1273 [26:26<23:14,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▍    | 687/1273 [26:28<22:52,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▍    | 688/1273 [26:30<21:52,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▍    | 689/1273 [26:33<23:15,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▍    | 690/1273 [26:34<21:46,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▍    | 691/1273 [26:37<22:31,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▍    | 692/1273 [26:39<21:28,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▍    | 693/1273 [26:42<23:07,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▍    | 694/1273 [26:44<22:44,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▍    | 695/1273 [26:47<25:06,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▍    | 696/1273 [26:49<24:10,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▍    | 697/1273 [26:52<24:52,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▍    | 698/1273 [26:55<24:32,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▍    | 699/1273 [26:57<23:54,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▍    | 700/1273 [26:59<22:27,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▌    | 701/1273 [27:01<20:59,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▌    | 702/1273 [27:04<23:39,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▌    | 703/1273 [27:06<23:06,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▌    | 704/1273 [27:08<21:46,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▌    | 705/1273 [27:10<20:55,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▌    | 706/1273 [27:14<23:29,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 707/1273 [27:16<22:47,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 708/1273 [27:18<23:20,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 709/1273 [27:21<22:33,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 710/1273 [27:23<22:46,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 711/1273 [27:25<21:28,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 712/1273 [27:27<21:14,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 713/1273 [27:30<21:46,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 714/1273 [27:32<21:29,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 715/1273 [27:34<20:37,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 716/1273 [27:36<19:59,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▋    | 717/1273 [27:39<21:01,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▋    | 718/1273 [27:41<20:28,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▋    | 719/1273 [27:42<18:56,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 720/1273 [27:44<18:54,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 721/1273 [27:47<19:24,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 722/1273 [27:48<18:17,  1.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 723/1273 [27:50<17:58,  1.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 724/1273 [27:52<18:14,  1.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 725/1273 [27:55<18:53,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 726/1273 [27:56<18:14,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 727/1273 [27:58<17:51,  1.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 728/1273 [28:01<19:23,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 729/1273 [28:03<18:30,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 730/1273 [28:05<20:20,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 731/1273 [28:07<19:51,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 732/1273 [28:10<20:04,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 733/1273 [28:12<19:02,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 734/1273 [28:14<18:52,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 735/1273 [28:16<19:49,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 736/1273 [28:18<19:29,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 737/1273 [28:20<19:03,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 738/1273 [28:22<18:52,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 739/1273 [28:25<19:48,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 740/1273 [28:27<20:16,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 741/1273 [28:30<21:07,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 742/1273 [28:32<20:02,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 743/1273 [28:34<19:18,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 744/1273 [28:36<17:56,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▊    | 745/1273 [28:37<16:40,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▊    | 746/1273 [28:39<16:54,  1.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▊    | 747/1273 [28:41<17:48,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▉    | 748/1273 [28:43<17:51,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▉    | 749/1273 [28:45<17:14,  1.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▉    | 750/1273 [28:47<17:30,  2.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▉    | 751/1273 [28:51<20:53,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▉    | 752/1273 [28:53<20:35,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▉    | 753/1273 [28:56<21:27,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▉    | 754/1273 [28:58<20:06,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▉    | 755/1273 [29:00<19:30,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▉    | 756/1273 [29:02<20:06,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▉    | 757/1273 [29:05<19:58,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|█████▉    | 758/1273 [29:07<19:02,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|█████▉    | 759/1273 [29:09<18:42,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|█████▉    | 760/1273 [29:11<18:58,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|█████▉    | 761/1273 [29:13<19:30,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|█████▉    | 762/1273 [29:15<18:21,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|█████▉    | 763/1273 [29:17<17:56,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|██████    | 764/1273 [29:20<19:34,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|██████    | 765/1273 [29:22<19:32,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|██████    | 766/1273 [29:25<19:50,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|██████    | 767/1273 [29:27<19:56,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|██████    | 768/1273 [29:30<20:19,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|██████    | 769/1273 [29:32<20:40,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|██████    | 770/1273 [29:35<20:23,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████    | 771/1273 [29:37<19:12,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████    | 772/1273 [29:39<19:41,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████    | 773/1273 [29:42<20:43,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████    | 774/1273 [29:44<20:53,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████    | 775/1273 [29:47<19:47,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████    | 776/1273 [29:49<20:18,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████    | 777/1273 [29:52<20:59,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████    | 778/1273 [29:54<20:57,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████    | 779/1273 [29:56<19:03,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████▏   | 780/1273 [29:58<18:10,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████▏   | 781/1273 [30:01<18:22,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████▏   | 782/1273 [30:03<18:23,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 783/1273 [30:05<17:43,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 784/1273 [30:07<18:35,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 785/1273 [30:09<18:03,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 786/1273 [30:12<18:11,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 787/1273 [30:14<17:32,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 788/1273 [30:16<17:58,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 789/1273 [30:18<17:31,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 790/1273 [30:20<16:42,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 791/1273 [30:22<16:31,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 792/1273 [30:24<17:23,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 793/1273 [30:26<16:38,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 794/1273 [30:28<16:21,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 795/1273 [30:30<15:57,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 796/1273 [30:32<15:37,  1.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 797/1273 [30:34<16:23,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 798/1273 [30:36<16:14,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 799/1273 [30:39<18:22,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 800/1273 [30:42<18:15,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 801/1273 [30:43<17:00,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 802/1273 [30:45<16:44,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 803/1273 [30:47<16:07,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 804/1273 [30:49<15:10,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 805/1273 [30:52<17:40,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 806/1273 [30:55<18:42,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 807/1273 [30:58<19:47,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 808/1273 [31:00<18:42,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▎   | 809/1273 [31:02<18:15,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▎   | 810/1273 [31:05<20:31,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▎   | 811/1273 [31:07<18:55,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▍   | 812/1273 [31:09<17:29,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▍   | 813/1273 [31:11<17:23,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▍   | 814/1273 [31:14<18:51,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▍   | 815/1273 [31:17<18:30,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▍   | 816/1273 [31:19<18:29,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▍   | 817/1273 [31:22<18:24,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▍   | 818/1273 [31:23<16:58,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▍   | 819/1273 [31:25<16:21,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▍   | 820/1273 [31:27<15:39,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▍   | 821/1273 [31:30<17:13,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▍   | 822/1273 [31:33<19:03,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▍   | 823/1273 [31:35<17:25,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▍   | 824/1273 [31:37<17:18,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▍   | 825/1273 [31:40<17:41,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▍   | 826/1273 [31:42<17:24,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▍   | 827/1273 [31:45<18:14,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▌   | 828/1273 [31:47<18:53,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▌   | 829/1273 [31:49<17:20,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▌   | 830/1273 [31:51<16:32,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▌   | 831/1273 [31:53<15:34,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▌   | 832/1273 [31:56<16:25,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▌   | 833/1273 [31:58<17:20,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 834/1273 [32:01<18:02,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 835/1273 [32:03<16:44,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 836/1273 [32:05<16:10,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 837/1273 [32:08<17:03,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 838/1273 [32:10<16:51,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 839/1273 [32:12<16:01,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 840/1273 [32:14<16:23,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 841/1273 [32:17<17:30,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 842/1273 [32:19<16:40,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 843/1273 [32:21<15:51,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▋   | 844/1273 [32:23<16:16,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▋   | 845/1273 [32:25<14:57,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▋   | 846/1273 [32:27<14:46,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 847/1273 [32:30<15:22,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 848/1273 [32:32<15:53,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 849/1273 [32:35<16:43,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 850/1273 [32:37<17:16,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 851/1273 [32:40<17:09,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 852/1273 [32:42<17:41,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 853/1273 [32:45<18:03,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 854/1273 [32:48<18:36,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 855/1273 [32:50<17:45,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 856/1273 [32:52<16:38,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 857/1273 [32:54<15:48,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 858/1273 [32:56<15:11,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 859/1273 [32:58<14:53,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 860/1273 [33:01<16:15,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 861/1273 [33:04<16:39,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 862/1273 [33:06<15:50,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 863/1273 [33:08<15:48,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 864/1273 [33:11<16:15,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 865/1273 [33:13<15:37,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 866/1273 [33:15<15:33,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 867/1273 [33:17<14:55,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 868/1273 [33:19<15:05,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 869/1273 [33:22<15:08,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 870/1273 [33:24<15:06,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 871/1273 [33:26<15:22,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 872/1273 [33:28<14:43,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▊   | 873/1273 [33:30<13:58,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▊   | 874/1273 [33:32<14:20,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▊   | 875/1273 [33:35<15:13,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 876/1273 [33:38<15:31,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 877/1273 [33:40<14:51,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 878/1273 [33:42<15:18,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 879/1273 [33:44<14:18,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 880/1273 [33:47<15:56,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 881/1273 [33:49<15:04,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 882/1273 [33:51<14:29,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 883/1273 [33:53<14:11,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 884/1273 [33:55<13:49,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|██████▉   | 885/1273 [33:57<14:01,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|██████▉   | 886/1273 [34:00<15:07,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|██████▉   | 887/1273 [34:02<14:27,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|██████▉   | 888/1273 [34:04<13:37,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|██████▉   | 889/1273 [34:06<13:29,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|██████▉   | 890/1273 [34:08<13:16,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|██████▉   | 891/1273 [34:10<13:06,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|███████   | 892/1273 [34:13<14:46,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|███████   | 893/1273 [34:15<14:36,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|███████   | 894/1273 [34:17<13:41,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|███████   | 895/1273 [34:19<13:53,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|███████   | 896/1273 [34:21<13:11,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|███████   | 897/1273 [34:23<12:21,  1.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████   | 898/1273 [34:25<13:07,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████   | 899/1273 [34:28<13:26,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████   | 900/1273 [34:30<14:33,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████   | 901/1273 [34:33<15:19,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████   | 902/1273 [34:35<14:54,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████   | 903/1273 [34:37<13:27,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████   | 904/1273 [34:40<15:14,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████   | 905/1273 [34:42<14:40,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████   | 906/1273 [34:45<15:37,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████   | 907/1273 [34:49<16:45,  2.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████▏  | 908/1273 [34:52<17:29,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████▏  | 909/1273 [34:54<16:41,  2.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████▏  | 910/1273 [34:56<15:46,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 911/1273 [34:59<15:56,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 912/1273 [35:02<16:54,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 913/1273 [35:05<16:32,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 914/1273 [35:07<15:41,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 915/1273 [35:10<15:59,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 916/1273 [35:12<14:32,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 917/1273 [35:14<13:43,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 918/1273 [35:16<13:49,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 919/1273 [35:18<13:11,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 920/1273 [35:21<12:53,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 921/1273 [35:23<13:36,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 922/1273 [35:26<13:53,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 923/1273 [35:28<12:59,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 924/1273 [35:30<14:13,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 925/1273 [35:33<13:51,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 926/1273 [35:35<14:24,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 927/1273 [35:38<14:35,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 928/1273 [35:40<13:38,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 929/1273 [35:42<13:21,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 930/1273 [35:45<13:37,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 931/1273 [35:47<13:39,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 932/1273 [35:50<13:29,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 933/1273 [35:52<13:43,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 934/1273 [35:54<13:04,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 935/1273 [35:56<12:19,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▎  | 936/1273 [35:59<12:46,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▎  | 937/1273 [36:01<13:32,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▎  | 938/1273 [36:03<11:46,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▍  | 939/1273 [36:05<11:31,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▍  | 940/1273 [36:07<11:21,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▍  | 941/1273 [36:10<12:37,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▍  | 942/1273 [36:12<13:02,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▍  | 943/1273 [36:14<12:50,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▍  | 944/1273 [36:16<12:03,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▍  | 945/1273 [36:18<11:30,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▍  | 946/1273 [36:20<11:48,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▍  | 947/1273 [36:22<11:17,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▍  | 948/1273 [36:24<10:55,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▍  | 949/1273 [36:26<10:51,  2.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▍  | 950/1273 [36:29<11:38,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▍  | 951/1273 [36:31<11:51,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▍  | 952/1273 [36:34<12:28,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▍  | 953/1273 [36:36<12:39,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▍  | 954/1273 [36:38<11:58,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▌  | 955/1273 [36:41<12:17,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▌  | 956/1273 [36:43<11:45,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▌  | 957/1273 [36:44<10:50,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▌  | 958/1273 [36:46<10:41,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▌  | 959/1273 [36:52<15:52,  3.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▌  | 960/1273 [36:54<14:59,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▌  | 961/1273 [36:57<14:38,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 962/1273 [36:59<13:16,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 963/1273 [37:01<13:30,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 964/1273 [37:04<12:57,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 965/1273 [37:06<12:04,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 966/1273 [37:08<11:28,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 967/1273 [37:10<11:49,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 968/1273 [37:13<12:00,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 969/1273 [37:15<11:23,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 970/1273 [37:17<11:22,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▋  | 971/1273 [37:18<10:20,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▋  | 972/1273 [37:20<10:13,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▋  | 973/1273 [37:23<10:56,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 974/1273 [37:25<11:03,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 975/1273 [37:28<12:10,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 976/1273 [37:31<12:24,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 977/1273 [37:33<12:03,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 978/1273 [37:36<12:20,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 979/1273 [37:38<11:32,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 980/1273 [37:41<12:46,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 981/1273 [37:44<13:32,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 982/1273 [37:49<15:45,  3.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 983/1273 [37:51<14:03,  2.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 984/1273 [37:53<13:02,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 985/1273 [37:56<12:53,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 986/1273 [37:58<11:49,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 987/1273 [38:00<12:19,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 988/1273 [38:03<12:43,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 989/1273 [38:06<13:06,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 990/1273 [38:08<11:59,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 991/1273 [38:10<11:07,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 992/1273 [38:13<11:55,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 993/1273 [38:16<12:31,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 994/1273 [38:19<12:05,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 995/1273 [38:21<11:02,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 996/1273 [38:23<11:21,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 997/1273 [38:25<10:46,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 998/1273 [38:27<10:14,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 999/1273 [38:29<09:39,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▊  | 1000/1273 [38:31<09:48,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▊  | 1001/1273 [38:34<09:56,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▊  | 1002/1273 [38:35<09:23,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 1003/1273 [38:39<10:49,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 1004/1273 [38:41<11:23,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 1005/1273 [38:44<11:44,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 1006/1273 [38:46<10:38,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 1007/1273 [38:48<10:05,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 1008/1273 [38:51<10:23,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 1009/1273 [38:54<12:09,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 1010/1273 [38:56<10:54,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 1011/1273 [38:58<10:33,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 1012/1273 [39:00<09:57,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|███████▉  | 1013/1273 [39:02<09:05,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|███████▉  | 1014/1273 [39:04<08:56,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|███████▉  | 1015/1273 [39:06<09:08,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|███████▉  | 1016/1273 [39:08<09:02,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|███████▉  | 1017/1273 [39:11<09:08,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|███████▉  | 1018/1273 [39:13<08:47,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████  | 1019/1273 [39:15<09:19,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████  | 1020/1273 [39:17<09:33,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████  | 1021/1273 [39:19<09:01,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████  | 1022/1273 [39:21<08:48,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████  | 1023/1273 [39:23<08:31,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████  | 1024/1273 [39:26<08:46,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 1025/1273 [39:28<08:53,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 1026/1273 [39:30<08:39,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 1027/1273 [39:31<08:04,  1.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 1028/1273 [39:33<07:52,  1.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 1029/1273 [39:35<07:45,  1.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 1030/1273 [39:37<08:04,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 1031/1273 [39:40<09:15,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 1032/1273 [39:43<09:28,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 1033/1273 [39:45<09:21,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 1034/1273 [39:47<09:14,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████▏ | 1035/1273 [39:51<10:15,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████▏ | 1036/1273 [39:54<10:40,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████▏ | 1037/1273 [39:56<09:50,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 1038/1273 [39:58<09:14,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 1039/1273 [40:00<09:07,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 1040/1273 [40:02<08:26,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 1041/1273 [40:04<08:30,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 1042/1273 [40:06<08:04,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 1043/1273 [40:07<07:31,  1.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 1044/1273 [40:09<07:32,  1.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 1045/1273 [40:12<08:28,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 1046/1273 [40:15<08:48,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 1047/1273 [40:17<09:05,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 1048/1273 [40:19<08:36,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 1049/1273 [40:22<09:18,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 1050/1273 [40:25<09:01,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 1051/1273 [40:27<08:30,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 1052/1273 [40:29<08:08,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 1053/1273 [40:31<07:59,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 1054/1273 [40:33<08:20,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 1055/1273 [40:35<07:57,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 1056/1273 [40:38<08:36,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 1057/1273 [40:41<08:42,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 1058/1273 [40:43<08:13,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 1059/1273 [40:45<07:43,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 1060/1273 [40:47<07:34,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 1061/1273 [40:49<07:27,  2.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 1062/1273 [40:50<07:03,  2.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▎ | 1063/1273 [40:52<07:03,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▎ | 1064/1273 [40:55<07:29,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▎ | 1065/1273 [40:57<07:35,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▎ | 1066/1273 [40:59<07:36,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▍ | 1067/1273 [41:02<08:03,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▍ | 1068/1273 [41:05<08:18,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▍ | 1069/1273 [41:07<07:49,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▍ | 1070/1273 [41:09<07:19,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▍ | 1071/1273 [41:11<07:07,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▍ | 1072/1273 [41:13<06:59,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▍ | 1073/1273 [41:15<07:07,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▍ | 1074/1273 [41:17<07:25,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▍ | 1075/1273 [41:19<07:10,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▍ | 1076/1273 [41:21<06:47,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▍ | 1077/1273 [41:23<06:40,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▍ | 1078/1273 [41:25<06:34,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▍ | 1079/1273 [41:28<06:57,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▍ | 1080/1273 [41:30<07:14,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▍ | 1081/1273 [41:32<07:15,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▍ | 1082/1273 [41:35<07:48,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▌ | 1083/1273 [41:37<07:21,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▌ | 1084/1273 [41:39<07:04,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▌ | 1085/1273 [41:42<07:05,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▌ | 1086/1273 [41:44<06:48,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▌ | 1087/1273 [41:46<07:09,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▌ | 1088/1273 [41:48<06:48,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 1089/1273 [41:51<06:49,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 1090/1273 [41:53<07:16,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 1091/1273 [41:55<06:53,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 1092/1273 [41:58<07:12,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 1093/1273 [42:00<07:15,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 1094/1273 [42:03<07:21,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 1095/1273 [42:07<08:19,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 1096/1273 [42:09<08:03,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 1097/1273 [42:11<07:13,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▋ | 1098/1273 [42:13<07:01,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▋ | 1099/1273 [42:15<06:41,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▋ | 1100/1273 [42:18<06:42,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▋ | 1101/1273 [42:20<06:52,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 1102/1273 [42:23<06:45,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 1103/1273 [42:24<06:15,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 1104/1273 [42:27<06:29,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 1105/1273 [42:29<06:04,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 1106/1273 [42:30<05:36,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 1107/1273 [42:33<05:48,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 1108/1273 [42:35<05:45,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 1109/1273 [42:37<05:51,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 1110/1273 [42:40<06:07,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 1111/1273 [42:41<05:37,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 1112/1273 [42:44<05:55,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 1113/1273 [42:46<05:44,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 1114/1273 [42:50<07:11,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 1115/1273 [42:52<06:34,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 1116/1273 [42:54<06:01,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 1117/1273 [42:56<05:45,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 1118/1273 [42:59<06:11,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 1119/1273 [43:01<06:26,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 1120/1273 [43:04<06:24,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 1121/1273 [43:06<06:03,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 1122/1273 [43:08<05:38,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 1123/1273 [43:10<05:55,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 1124/1273 [43:12<05:36,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 1125/1273 [43:15<05:53,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 1126/1273 [43:17<05:33,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▊ | 1127/1273 [43:20<05:40,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▊ | 1128/1273 [43:23<06:03,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▊ | 1129/1273 [43:24<05:25,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▉ | 1130/1273 [43:27<05:38,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▉ | 1131/1273 [43:29<05:08,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▉ | 1132/1273 [43:31<05:20,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▉ | 1133/1273 [43:34<05:26,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▉ | 1134/1273 [43:36<05:49,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▉ | 1135/1273 [43:39<05:41,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▉ | 1136/1273 [43:41<05:41,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▉ | 1137/1273 [43:43<05:14,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▉ | 1138/1273 [43:45<05:03,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▉ | 1139/1273 [43:51<07:25,  3.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████▉ | 1140/1273 [43:53<06:28,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████▉ | 1141/1273 [43:55<06:00,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████▉ | 1142/1273 [43:58<05:41,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████▉ | 1143/1273 [44:00<05:39,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████▉ | 1144/1273 [44:03<05:31,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████▉ | 1145/1273 [44:05<05:09,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|█████████ | 1146/1273 [44:07<04:53,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|█████████ | 1147/1273 [44:10<04:59,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|█████████ | 1148/1273 [44:12<04:54,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|█████████ | 1149/1273 [44:14<04:39,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|█████████ | 1150/1273 [44:16<04:27,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|█████████ | 1151/1273 [44:19<04:50,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|█████████ | 1152/1273 [44:21<04:29,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████ | 1153/1273 [44:23<04:22,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████ | 1154/1273 [44:26<04:55,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████ | 1155/1273 [44:28<04:38,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████ | 1156/1273 [44:30<04:23,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████ | 1157/1273 [44:33<04:42,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████ | 1158/1273 [44:35<04:36,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████ | 1159/1273 [44:37<04:15,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████ | 1160/1273 [44:40<04:27,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████ | 1161/1273 [44:42<04:31,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████▏| 1162/1273 [44:44<04:17,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████▏| 1163/1273 [44:47<04:19,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████▏| 1164/1273 [44:49<04:18,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 1165/1273 [44:51<04:12,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 1166/1273 [44:53<04:01,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 1167/1273 [44:56<04:06,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 1168/1273 [44:58<04:08,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 1169/1273 [45:01<04:09,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 1170/1273 [45:04<04:17,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 1171/1273 [45:06<03:58,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 1172/1273 [45:08<04:01,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 1173/1273 [45:10<03:41,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 1174/1273 [45:13<03:56,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 1175/1273 [45:16<04:11,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 1176/1273 [45:18<03:52,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 1177/1273 [45:20<03:37,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 1178/1273 [45:22<03:49,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 1179/1273 [45:25<03:55,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 1180/1273 [45:28<04:02,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 1181/1273 [45:30<03:43,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 1182/1273 [45:33<03:55,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 1183/1273 [45:35<03:40,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 1184/1273 [45:37<03:32,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 1185/1273 [45:40<03:28,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 1186/1273 [45:41<03:11,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 1187/1273 [45:45<03:42,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 1188/1273 [45:48<03:43,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 1189/1273 [45:50<03:25,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 1190/1273 [45:52<03:31,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▎| 1191/1273 [45:55<03:31,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▎| 1192/1273 [45:57<03:17,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▎| 1193/1273 [46:00<03:22,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▍| 1194/1273 [46:02<03:15,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▍| 1195/1273 [46:04<03:03,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▍| 1196/1273 [46:06<02:54,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▍| 1197/1273 [46:08<02:46,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▍| 1198/1273 [46:11<02:45,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▍| 1199/1273 [46:13<02:44,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▍| 1200/1273 [46:16<02:52,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▍| 1201/1273 [46:18<02:58,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▍| 1202/1273 [46:20<02:42,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▍| 1203/1273 [46:23<02:50,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▍| 1204/1273 [46:25<02:35,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▍| 1205/1273 [46:27<02:29,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▍| 1206/1273 [46:29<02:29,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▍| 1207/1273 [46:31<02:19,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▍| 1208/1273 [46:33<02:20,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▍| 1209/1273 [46:37<02:42,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▌| 1210/1273 [46:39<02:29,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▌| 1211/1273 [46:41<02:16,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▌| 1212/1273 [46:44<02:32,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▌| 1213/1273 [46:46<02:21,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▌| 1214/1273 [46:48<02:21,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▌| 1215/1273 [46:51<02:26,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 1216/1273 [46:53<02:15,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 1217/1273 [46:55<02:12,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 1218/1273 [46:57<02:00,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 1219/1273 [46:59<01:56,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 1220/1273 [47:02<01:56,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 1221/1273 [47:03<01:49,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 1222/1273 [47:06<01:52,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 1223/1273 [47:09<02:06,  2.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 1224/1273 [47:11<01:51,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 1225/1273 [47:13<01:51,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▋| 1226/1273 [47:15<01:45,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▋| 1227/1273 [47:18<01:48,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▋| 1228/1273 [47:21<01:48,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 1229/1273 [47:23<01:44,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 1230/1273 [47:25<01:40,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 1231/1273 [47:27<01:34,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 1232/1273 [47:30<01:36,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 1233/1273 [47:32<01:29,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 1234/1273 [47:34<01:30,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 1235/1273 [47:36<01:20,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 1236/1273 [47:38<01:18,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 1237/1273 [47:41<01:22,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 1238/1273 [47:43<01:20,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 1239/1273 [47:45<01:19,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 1240/1273 [47:48<01:15,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 1241/1273 [47:50<01:10,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 1242/1273 [47:52<01:09,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 1243/1273 [47:54<01:08,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 1244/1273 [47:57<01:06,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 1245/1273 [47:59<01:06,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 1246/1273 [48:02<01:05,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 1247/1273 [48:04<01:04,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 1248/1273 [48:06<00:58,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 1249/1273 [48:08<00:52,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 1250/1273 [48:10<00:50,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 1251/1273 [48:12<00:47,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 1252/1273 [48:15<00:49,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 1253/1273 [48:17<00:45,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▊| 1254/1273 [48:20<00:44,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▊| 1255/1273 [48:22<00:40,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▊| 1256/1273 [48:24<00:37,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▊| 1257/1273 [48:27<00:39,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▉| 1258/1273 [48:29<00:34,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▉| 1259/1273 [48:31<00:31,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▉| 1260/1273 [48:33<00:28,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▉| 1261/1273 [48:35<00:24,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▉| 1262/1273 [48:37<00:23,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▉| 1263/1273 [48:40<00:21,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▉| 1264/1273 [48:42<00:19,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▉| 1265/1273 [48:44<00:17,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▉| 1266/1273 [48:46<00:14,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|█████████▉| 1267/1273 [48:48<00:12,  2.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|█████████▉| 1268/1273 [48:50<00:10,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|█████████▉| 1269/1273 [48:52<00:08,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|█████████▉| 1270/1273 [48:55<00:07,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|█████████▉| 1271/1273 [48:57<00:04,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|█████████▉| 1272/1273 [49:00<00:02,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 1273/1273 [49:00<00:00,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "responses = []\n",
    "answers = []\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    questions, options_strs, answer_idxs = batch\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    prompts = [prompt_context.format(question=question, options=options_str) for question, options_str in zip(questions, options_strs)]\n",
    "    \n",
    "    inputs = tokenizer(prompts, return_tensors='pt', padding=True, truncation=True, max_length=1500).to(model_llama.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_llama.generate(**inputs,max_new_tokens=1)\n",
    "    \n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    #print(decoded_outputs)\n",
    "    for decoded_output, answer in zip(decoded_outputs, answer_idxs):\n",
    "        position = decoded_output.find('#Answer:')\n",
    "        answer_pred = decoded_output[position+8 :position+10].strip()\n",
    "        #print(answer_pred)\n",
    "        if answer == answer_pred.strip():\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        responses.append(answer_pred)\n",
    "        answers.append(answer)\n",
    "        total_predictions += 1\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T12:12:01.625020Z",
     "iopub.status.busy": "2024-07-12T12:12:01.624329Z",
     "iopub.status.idle": "2024-07-12T12:12:01.631109Z",
     "shell.execute_reply": "2024-07-12T12:12:01.630229Z",
     "shell.execute_reply.started": "2024-07-12T12:12:01.624959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.18%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6634"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Accuracy: {correct_predictions / len(responses):.2%}\")\n",
    "correct_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Batches with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T12:12:07.276303Z",
     "iopub.status.busy": "2024-07-12T12:12:07.275645Z",
     "iopub.status.idle": "2024-07-12T12:12:07.282846Z",
     "shell.execute_reply": "2024-07-12T12:12:07.281898Z",
     "shell.execute_reply.started": "2024-07-12T12:12:07.276271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10178"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "responses = []\n",
    "answers = []\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    questions, options_strs, answer_idxs, combined_contexts = batch\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    prompts = [prompt_context.format(question=question, options=options_str, context=combined_context) for question, options_str,combined_context in zip(questions, options_strs,combined_contexts)]\n",
    "    \n",
    "    inputs = tokenizer(prompts, return_tensors='pt', padding=True, truncation=True, max_length=4000).to(model_llama.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_llama.generate(**inputs,max_new_tokens=1)\n",
    "    \n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    #print(decoded_outputs)\n",
    "    for decoded_output, answer in zip(decoded_outputs, answer_idxs):\n",
    "        position = decoded_output.find('#Answer:')\n",
    "        answer_pred = decoded_output[position+8 :position+10].strip()\n",
    "        #print(answer_pred)\n",
    "        if answer == answer_pred.strip():\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        responses.append(answer_pred)\n",
    "        answers.append(answer)\n",
    "        total_predictions += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5259168,
     "sourceId": 8754656,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 3093,
     "sourceId": 4298,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 27739,
     "sourceId": 33146,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
